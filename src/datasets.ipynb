{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "This notebook standardizes three moral psychology datasets (MFRC, MFTC, eMFD) for comparative LLM evaluation:\n",
    "\n",
    "Key Processing Steps:\n",
    "1. Label standardization to unified 5-foundation taxonomy\n",
    "2. Text deduplication with unique text_id assignment\n",
    "3. Multi-annotator format preservation\n",
    "4. Upload to HuggingFace Hub for reproducible access\n",
    "\n",
    "Output: Three clean datasets (user/morality-{MFRC,MFTC,eMFD}) ready for zero-shot moral classification evaluation across different text domains\n",
    "(social media, news, forums)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from itertools import islice\n",
    "from dotenv import load_dotenv\n",
    "from datasets import Dataset, DatasetDict\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MFRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"USC-MOLA-Lab/MFRC\")\n",
    "df = pd.DataFrame( ds['train'] )\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"annotation\"].str.split(\",\").explode().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_map = {\n",
    "    \"Thin Morality\": \"none\",\n",
    "    \"Non-Moral\": \"none\",\n",
    "    \"Care\": \"care\",\n",
    "    \"Purity\": \"sanctity\",\n",
    "    \"Authority\": \"authority\",\n",
    "    \"Loyalty\": \"loyalty\",\n",
    "    \"Proportionality\": \"fairness\",\n",
    "    \"Equality\": \"fairness\"\n",
    "}\n",
    "\n",
    "df[\"label\"] = df[\"annotation\"].replace(key_map,regex=True) # to allow for substrings\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_id'] = df.groupby('text').ngroup()\n",
    "assert df['text_id'].max() == df['text_id'].nunique()-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "ds = Dataset.from_pandas(df)\n",
    "ds.push_to_hub(\"maciejskorski/morality-MFRC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MFTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(\n",
    "   data=pd.read_json('../data/MFTC/MFTC_V4_text.json').to_dict('records'),\n",
    "   record_path=['Tweets', 'annotations'],\n",
    "   meta=[\n",
    "       ['Corpus'],\n",
    "       ['Tweets', 'tweet_id'],\n",
    "       ['Tweets', 'tweet_text']\n",
    "   ]\n",
    ")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['annotation'].str.split(\",\").explode().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_map = {\n",
    "        'care': 'care',\n",
    "        'harm': 'care',\n",
    "        'fairness': 'fairness',\n",
    "        'cheating': 'fairness',\n",
    "        'loyalty': 'loyalty',\n",
    "        'betrayal': 'loyalty',\n",
    "        'authority': 'authority',\n",
    "        'subversion': 'authority',\n",
    "        'purity': 'sanctity',\n",
    "        'degradation': 'sanctity'\n",
    "}\n",
    "\n",
    "df['label'] = df['annotation'].replace(key_map,regex=True)\n",
    "df = df.rename({'Tweets.tweet_id':'tweet_id','Tweets.tweet_text':'text',},axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].str.split(\",\").explode().unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_id'] = df.groupby('text').ngroup()\n",
    "assert df['text_id'].max() == df['text_id'].nunique()-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.from_pandas(df)\n",
    "ds.push_to_hub(\"maciejskorski/morality-MFTC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMFD\n",
    "\n",
    "Source https://osf.io/vw85e/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('highlights_raw.csv')\n",
    "df.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['assigned_domain'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_id'] = df.groupby('content').ngroup()\n",
    "df.rename({'content':'text','coder_id':'annotator'},axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moral_targets = ['care', 'fairness', 'loyalty', 'authority', 'sanctity']\n",
    "assert set(df['assigned_domain'].unique()) == set(moral_targets)\n",
    "df.rename({'assigned_domain':'label'},axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.from_pandas(df)\n",
    "ds.push_to_hub(\"maciejskorski/morality-eMFD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics Summary\n",
      "==================================================\n",
      "\n",
      "### MORALITY-MFRC\n",
      "Total annotations: 61,226\n",
      "Unique texts: 17,886\n",
      "Avg annotations per text: 3.4\n",
      "\n",
      "Moral Foundation Prevalence:\n",
      "  care: 4,740 texts (26.5%)\n",
      "  fairness: 5,280 texts (29.5%)\n",
      "  loyalty: 1,977 texts (11.1%)\n",
      "  authority: 3,430 texts (19.2%)\n",
      "  sanctity: 1,747 texts (9.8%)\n",
      "----------------------------------------\n",
      "\n",
      "### MORALITY-MFTC\n",
      "Total annotations: 128,454\n",
      "Unique texts: 33,687\n",
      "Avg annotations per text: 3.8\n",
      "\n",
      "Moral Foundation Prevalence:\n",
      "  care: 13,716 texts (40.7%)\n",
      "  fairness: 11,982 texts (35.6%)\n",
      "  loyalty: 10,305 texts (30.6%)\n",
      "  authority: 11,280 texts (33.5%)\n",
      "  sanctity: 7,543 texts (22.4%)\n",
      "----------------------------------------\n",
      "\n",
      "### MORALITY-EMFD\n",
      "Total annotations: 73,001\n",
      "Unique texts: 54,868\n",
      "Avg annotations per text: 1.3\n",
      "\n",
      "Moral Foundation Prevalence:\n",
      "  care: 13,438 texts (24.5%)\n",
      "  fairness: 12,742 texts (23.2%)\n",
      "  loyalty: 12,714 texts (23.2%)\n",
      "  authority: 13,609 texts (24.8%)\n",
      "  sanctity: 10,308 texts (18.8%)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Dataset names\n",
    "datasets = ['morality-MFRC', 'morality-MFTC', 'morality-eMFD']\n",
    "moral_foundations = ['care', 'fairness', 'loyalty', 'authority', 'sanctity']\n",
    "\n",
    "print(\"Dataset Statistics Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for dataset_name in datasets:\n",
    "    print(f\"\\n### {dataset_name.upper()}\")\n",
    "    \n",
    "    # Load dataset\n",
    "    ds = load_dataset(f\"maciejskorski/{dataset_name}\")['train']\n",
    "    df = ds.to_pandas()\n",
    "    \n",
    "    # Basic statistics\n",
    "    n_annotations = len(df)\n",
    "    n_unique_texts = df['text_id'].nunique()\n",
    "    \n",
    "    print(f\"Total annotations: {n_annotations:,}\")\n",
    "    print(f\"Unique texts: {n_unique_texts:,}\")\n",
    "    print(f\"Avg annotations per text: {n_annotations/n_unique_texts:.1f}\")\n",
    "    \n",
    "    # Moral foundation prevalence using text.str.contains\n",
    "    print(\"\\nMoral Foundation Prevalence:\")\n",
    "    \n",
    "    # Group by text_id and aggregate labels\n",
    "    text_labels = df.groupby('text_id')['label'].apply(';'.join)\n",
    "    \n",
    "    for foundation in moral_foundations:\n",
    "        texts_with_foundation = text_labels.str.contains(foundation).sum()\n",
    "        pct = (texts_with_foundation / n_unique_texts) * 100\n",
    "        print(f\"  {foundation}: {texts_with_foundation:,} texts ({pct:.1f}%)\")\n",
    "    \n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
