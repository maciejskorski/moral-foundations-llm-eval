{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "This notebook evaluates AI model performance against human annotators in classifying moral foundations using Haidt's Moral Foundations Theory. \n",
    "\n",
    "Key Components:\n",
    "- Data: WandB experiment results\n",
    "- Method: variant of Dawid-Skene's competence model implemented in TensorFlow to estimate annotator competence and consensus\n",
    "- Metrics: True/false positive rates, balanced accuracy, and percentile rankings\n",
    "\n",
    "Main Findings: AI understands moral dimensions with more balanced accuracy, ranking 75th-100th percentile vs humans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Load WandB experiment results and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'demo': 'false',\n",
      " 'model_name': 'llama4_maverick',\n",
      " 'prompt': 'You are an expert in moral psychology, classifying text according '\n",
      "           \"to Haidt's theory.\\n\"\n",
      "           '            For each moral foundations, mark true if moral values '\n",
      "           'from that foundation are expressed in the text, false if not '\n",
      "           'expressed.\\n'\n",
      "           '\\n'\n",
      "           '            Answer only with a valid JSON in this format:\\n'\n",
      "           '            {\\n'\n",
      "           '                \"care/harm\": [true / false],\\n'\n",
      "           '                \"fairness/cheating\": [true / false],\\n'\n",
      "           '                \"loyalty/betrayal\": [true / false],\\n'\n",
      "           '                \"authority/subversion\": [true / false],\\n'\n",
      "           '                \"sanctity/degradation\": [true / false],\\n'\n",
      "           '            }\\n'\n",
      "           '            ',\n",
      " 'random_state': 13,\n",
      " 'sample': -1,\n",
      " 'temperature': 0.3,\n",
      " 'test_data': 'morality-MFRC'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "api = wandb.Api()\n",
    "\n",
    "# run_id = \"1f1usujv\" # claude-4-sonnet MFTC\n",
    "# run_id = \"pckcakff\"  # claude-4-sonnet MFTC\n",
    "# run_id = \"5kucsisw\" # deepseek MFTC\n",
    "# run_id = \"yon5adbv\" # deepseek MFTC\n",
    "# run_id = \"7krxewfl\" # deepseek eMFD\n",
    "# run_id = \"jpsu9gfg\" # deepseek MFRC\n",
    "# run_id = \"744jcvse\" # claude-4-sonnet MFRC\n",
    "# run_id = \"d1kvjg6q\" # claude-4-sonnet emfd\n",
    "# run_id = \"2fisp7sj\" # llama4_maverick MFTC\n",
    "# run_id = \"fq9kn2ok\" # llama4_maverick MFRC\n",
    "# run_id = \"jv4exac1\" # llama4_maverick emfd\n",
    "\n",
    "project_name = \"morality-llm\"\n",
    "run = api.run(f\"{project_name}/{run_id}\")\n",
    "pprint(run.config)\n",
    "artifact = api.artifact(f\"{project_name}/run-{run_id}-predictions:latest\")\n",
    "\n",
    "# modify the artifact, here add an index\n",
    "table = artifact.get(\"predictions\")\n",
    "table = pd.DataFrame(table.data,columns=table.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "pattern = re.compile(r':\\s*\\[?(true|false)\\]?', re.IGNORECASE)\n",
    "moral_targets = ['care', 'fairness', 'loyalty', 'authority', 'sanctity']\n",
    "\n",
    "def extract_booleans(text):\n",
    "   # Find true/false values in order\n",
    "   matches = re.findall(r':\\s*(true|false)', text.lower())[:5]\n",
    "   return [m == 'true' for m in matches]\n",
    "\n",
    "\n",
    "y_pred = table['predictions'].str.findall(pattern).str[:5].apply(lambda x: [s == 'true' for s in x])\n",
    "# y_pred = table['predictions'].apply(extract_booleans)\n",
    "y_pred = pd.DataFrame(y_pred.tolist(), columns=moral_targets, index=table['index'])\n",
    "y_pred.index.name = 'text_id'\n",
    "print(\"NaN\",y_pred.isna().mean())\n",
    "print(y_pred.shape)\n",
    "y_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from IPython.display import display\n",
    "test_data = run.config['test_data']\n",
    "ds_test = load_dataset(f\"maciejskorski/{test_data}\")['train']\n",
    "annots = ds_test.to_pandas().set_index('text_id')#.loc[test_idxs]\n",
    "# print(annots.groupby('Corpus')['tweet_id'].nunique())\n",
    "display(annots.head())\n",
    "\n",
    "annots = annots.pivot_table(\n",
    "    index=['text_id', 'text'], \n",
    "    columns='annotator', \n",
    "    values='label', \n",
    "    aggfunc='first' \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # annots = annots.join(y_pred[\"care\"])\n",
    "\n",
    "# target = 'sanctity'\n",
    "# y_preds =  pd.concat([annots[t].str.lower().str.contains(target) for t in annots.columns],axis=1)\n",
    "# y_preds = y_preds.join(y_pred[target].astype(bool),how='inner')\n",
    "# y_preds = y_preds.astype(float)\n",
    "# print(y_preds.shape)\n",
    "\n",
    "\n",
    "pattern = '|'.join(moral_targets)\n",
    "y_preds = annots.stack().str.lower().str.contains(pattern).fillna(0).unstack()\n",
    "y_preds = y_preds.astype(float)\n",
    "A = y_preds.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from itertools import combinations\n",
    "\n",
    "from itertools import combinations\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "def pabak(annotations):\n",
    "    obs_aggreements = []\n",
    "    for ann1, ann2 in combinations(range(annotations.shape[1]), 2):\n",
    "        y1, y2 = annotations[:,ann1],annotations[:,ann2]\n",
    "        mask = ~(np.isnan(y1) | np.isnan(y2))\n",
    "        po = np.mean(y1[mask] == y2[mask])\n",
    "        obs_aggreements.append( 2*po -1 )\n",
    "\n",
    "    obs_aggreements = np.array(obs_aggreements)\n",
    "    return np.nanmean(obs_aggreements)\n",
    "\n",
    "\n",
    "for target in moral_targets + ['any']:\n",
    "    pattern = target\n",
    "    if target == 'any':\n",
    "        pattern = '|'.join(moral_targets)\n",
    "        \n",
    "    # pattern = 'fairness'\n",
    "    # target = 'care'\n",
    "    y_preds = annots.stack().str.lower().str.contains(pattern).unstack()\n",
    "    y_preds = y_preds.astype(float)\n",
    "    A = y_preds.values\n",
    "    score = pabak(A)\n",
    "    \n",
    "    print(target, score )\n",
    "\n",
    "# A = y_preds.values\n",
    "# outs = []\n",
    "\n",
    "# for i in range(A.shape[1]):\n",
    "#     mask = np.ones(A.shape[1])==1\n",
    "#     mask[i] =  False\n",
    "#     avg_kappa = pabak(A[:,mask])\n",
    "#     outs.append( avg_kappa )\n",
    "#     print(f\"Average PABAK w/o {i}: {avg_kappa:.3f}\")\n",
    "# outs = np.array(outs)\n",
    "# outs.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Dawid Skene\n",
    "\n",
    "Implement an annotator competence estimation model in TensorFlow, a variation of Dawid-Skene's algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_probability as tfp\n",
    "from tqdm import tqdm\n",
    "\n",
    "def init(J,K):\n",
    "    pi_logits = tf.Variable(tf.random.normal([K]) * 0.1, name='pi_logits')\n",
    "    initial_theta = 0.6*tf.eye(K)  + 0.4/K\n",
    "    theta_logits = tf.Variable(\n",
    "        tf.math.log(tf.tile(initial_theta[None,:,:], [J,1,1]) + 1e-8),\n",
    "        name='theta_logits'\n",
    "    ) # annotator x true class x pred class\n",
    "    class_prior = tfp.distributions.Dirichlet(alpha,name='pi_prior')\n",
    "    confusion_alpha = tf.ones([J, K, K]) * 0.4\n",
    "    diag_values = tf.fill([J, K], 0.6)  # Shape [J, K] for J annotators, K classes\n",
    "    confusion_alpha = tf.linalg.set_diag(confusion_alpha, diag_values)\n",
    "    confusion_prior = tfp.distributions.Dirichlet(3*confusion_alpha, name='confusion_priors')\n",
    "    return pi_logits, theta_logits, class_prior, confusion_prior\n",
    "\n",
    "\n",
    "def log_p(pi_logits, theta_logits, annot_ids):\n",
    "    \"\"\"\n",
    "    Implements the log-likelihood computation for a Dawid-Skene competence model, \n",
    "    estimating both class prevalences and annotator confusion matrices.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    pi_logits : tf.Variable, shape [K]\n",
    "        Logits for class prevalence distribution π (before softmax normalization)\n",
    "    theta_logits : tf.Variable, shape [J, K, K] \n",
    "        Logits for annotator confusion matrices θ (before softmax normalization)\n",
    "        θ[j,i,k] = P(annotator j labels class k | true class i)\n",
    "    annot_ids : tf.SparseTensor, shape [N, J*K]\n",
    "        Sparse tensor encoding annotation observations where:\n",
    "        - indices: (item, annotator) pairs\n",
    "        - values: observed classes encoded for efficient embedding lookup\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tf.Tensor : scalar\n",
    "        Log-likelihood = log P(annotations | π, θ) + log P(π) + log P(θ)\n",
    "        Combines data likelihood with Dirichlet priors on π and θ\n",
    "        \n",
    "    Mathematical formulation:\n",
    "    ------------------------\n",
    "    log P(annotations) = Σᵢ log Σₖ π_k Πⱼ θⱼₖ,yᵢⱼ + log P(π) + Σⱼ log P(θⱼ)\n",
    "    where yᵢⱼ is the annotation by annotator j on item i\n",
    "    \"\"\"\n",
    "    log_pi = tf.nn.log_softmax(pi_logits)\n",
    "    log_theta = tf.nn.log_softmax(theta_logits, axis=-1) # [annotator x true class x pred class]\n",
    "    pi = tf.math.exp(log_pi)  # [true class]\n",
    "    theta = tf.math.exp(log_theta)\n",
    "    log_theta = tf.transpose(log_theta,[0,2,1]) \n",
    "    log_theta = tf.reshape(log_theta, (J*K,K)) # [annotator * true class, x pred class]\n",
    "    log_p = tf.nn.embedding_lookup_sparse(log_theta, annot_ids, sp_weights=None, combiner='sum')  # [items x true class]\n",
    "    log_p += log_pi[None, :]\n",
    "    log_p = tf.reduce_logsumexp(log_p, axis=1) # [items]\n",
    "    log_p = tf.reduce_sum(log_p)\n",
    "    \n",
    "    log_p += class_prior.log_prob(pi)\n",
    "    log_p += tf.reduce_sum(confusion_prior.log_prob(theta))\n",
    "    return log_p\n",
    "\n",
    "\n",
    "optimizer = tf.optimizers.Adam(1e-2, )\n",
    "max_iter = 2000\n",
    "\n",
    "\n",
    "@tf.function()\n",
    "def train_step(pi_logits, theta_logits, annot_ids):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = -log_p(pi_logits, theta_logits, annot_ids)\n",
    "    gradients = tape.gradient(loss, [pi_logits, theta_logits])\n",
    "    optimizer.apply_gradients(zip(gradients, [pi_logits, theta_logits]))\n",
    "    return loss\n",
    "\n",
    "\n",
    "@tf.function()\n",
    "def train(pi_logits, theta_logits, annot_ids, max_iter=tf.constant(1)):\n",
    "    print(\"tracing\")\n",
    "    for i in tf.range(max_iter):\n",
    "        loss = train_step(pi_logits, theta_logits, annot_ids )   \n",
    "\n",
    "true_labels = tf.random.uniform([10000], 0, 2, dtype=tf.int32)\n",
    "errors = tf.random.uniform([10000, 5]) < 0.25\n",
    "annotations = tf.where(errors, 1 - true_labels[:, None], true_labels[:, None]).numpy()\n",
    " \n",
    "N, J = annotations.shape\n",
    "K = 2\n",
    "alpha = [80,20]\n",
    "\n",
    "valid_mask = ~np.isnan(annotations) \n",
    "n_coords, j_coords = np.where(valid_mask)\n",
    "idxs = np.column_stack([n_coords, j_coords])\n",
    "idx_values = j_coords * K + annotations[valid_mask].astype(int)\n",
    "annot_ids = tf.SparseTensor(indices=idxs, values=idx_values, dense_shape=[N, J])\n",
    "\n",
    "pi_logits, theta_logits, class_prior, confusion_prior = init(J,K)\n",
    "\n",
    "for _ in range(3):\n",
    "    train(pi_logits, theta_logits, annot_ids)\n",
    "\n",
    "\n",
    "device = \"/GPU:0\"\n",
    "with tf.device(device):\n",
    "    train(pi_logits, theta_logits, annot_ids, max_iter=tf.constant(2000))\n",
    "\n",
    "\n",
    "theta = tf.nn.softmax(theta_logits, axis=-1)\n",
    "competences = tf.reduce_sum(tf.ones(K)*1.0/K * tf.linalg.diag_part(theta), axis=1).numpy()\n",
    "tf.debugging.assert_near(competences, 0.75, atol=2e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from scipy import stats\n",
    "import altair as alt\n",
    "\n",
    "moral_colors = {\n",
    "    'authority': '#6A4C93',\n",
    "    'care': '#00B4A6',\n",
    "    'fairness': '#3498DB',\n",
    "    'loyalty': '#E74C3C',\n",
    "    'sanctity': '#F39C12',\n",
    "    'any': 'gray'\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "charts = []\n",
    "for target in moral_targets + ['any']: # moral_targets+\n",
    "\n",
    "    if target == 'any':\n",
    "        y_pred_t = y_pred.any(axis=1)\n",
    "        pattern = '|'.join(moral_targets)\n",
    "    else:\n",
    "        y_pred_t = y_pred[target].astype(bool)\n",
    "        pattern = target\n",
    "    y_preds = annots.stack().str.lower().str.contains(pattern).unstack()\n",
    "    # y_preds =  annots.stack().str.lower().str.contains(target).unstack()\n",
    "    y_pred_t.name = run.config['model_name']\n",
    "    y_preds = y_preds.join(y_pred_t,how='inner')\n",
    "    y_preds = y_preds.astype(float)\n",
    "    annotations = y_preds.values\n",
    "\n",
    "    N, J = annotations.shape\n",
    "    K = int(np.nanmax(annotations))+1\n",
    "    print(N,J,K)\n",
    "    valid_mask = ~np.isnan(annotations) \n",
    "    n_coords, j_coords = np.where(valid_mask)\n",
    "    idxs = np.column_stack([n_coords, j_coords])\n",
    "    idx_values = j_coords * K + annotations[valid_mask].astype(int)\n",
    "    annot_ids = tf.SparseTensor(indices=idxs, values=idx_values, dense_shape=[N, J])\n",
    "\n",
    "    pi_logits, theta_logits, class_prior, confusion_prior = init(J,K)\n",
    "    if target=='any':\n",
    "        class_prior = tfp.distributions.Dirichlet([500,500],name='pi_prior')\n",
    "    optimizer = tf.optimizers.Adam(1e-2, )\n",
    "\n",
    "    @tf.function()\n",
    "    def train_step(pi_logits, theta_logits, annot_ids):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = -log_p(pi_logits, theta_logits, annot_ids)\n",
    "        gradients = tape.gradient(loss, [pi_logits, theta_logits])\n",
    "        optimizer.apply_gradients(zip(gradients, [pi_logits, theta_logits]))\n",
    "        return loss\n",
    "\n",
    "    device = \"/GPU:0\"\n",
    "    with tf.device(device):\n",
    "        for _ in tqdm(range(2000), total=2000):\n",
    "            train_step(pi_logits, theta_logits, annot_ids, )    \n",
    "\n",
    "    pi = tf.nn.softmax(pi_logits)\n",
    "    theta = tf.nn.softmax(theta_logits, axis=-1)\n",
    "\n",
    "    # competences = tf.reduce_sum(tf.ones(K)*1.0/K * tf.linalg.diag_part(theta), axis=1).numpy()\n",
    "\n",
    "    tpr = theta[:,1,1].numpy()\n",
    "    tnr = theta[:,0,0].numpy()\n",
    "\n",
    "    f11 = 2*tpr/(2-(1-tpr)+(1-tnr)*pi[0]/pi[1])\n",
    "    f10 = 2*tnr/(2-(1-tnr)+(1-tpr)*pi[1]/pi[0])\n",
    "    competences = (tpr+tnr)/2\n",
    "    \n",
    "    tpr_rank = stats.percentileofscore(tpr, tpr[-1])\n",
    "    tnr_rank = stats.percentileofscore(tnr, tnr[-1])\n",
    "    ba = (tpr+tnr)/2\n",
    "    ba_mean = ba.mean()\n",
    "    ba_rank = stats.percentileofscore(ba, ba[-1])\n",
    "    \n",
    "    results[target] = {\n",
    "        'tpr_ai_rank': tpr_rank,\n",
    "        'tnr_ai_rank': tnr_rank,\n",
    "        'tpr_mean': tpr.mean(),\n",
    "        'tnr_mean': tnr.mean(),\n",
    "        'ba_mean': ba.mean(),\n",
    "        'ba_ai_rank': ba_rank,\n",
    "        'tpr_ai': tpr[-1],\n",
    "        'tnr_ai': tnr[-1], \n",
    "        'ba_ai': ba[-1],\n",
    "        'pi_1': pi[1].numpy(),\n",
    "    }\n",
    "    \n",
    "    acc = pd.Series(data=competences,index=y_preds.columns)\n",
    "    acc.index.name = \"Annotator\"\n",
    "    acc.name = \"Accuracy\"\n",
    "\n",
    "    df = acc.sort_values().reset_index()\n",
    "    ai_pos = df[df['Annotator'] == acc.index[-1]].index[0]\n",
    "    df['Annotator'] = range(len(df))\n",
    "    df['Moral Dimension'] = target\n",
    "    df['type'] = 'Annotator'  # for human annotators\n",
    "    df.loc[ai_pos, 'type'] = 'AI'  # for AI row\n",
    "    mean_acc = acc.mean()\n",
    "    ai_percentile = stats.percentileofscore(df['Accuracy'], df.iloc[ai_pos]['Accuracy'])\n",
    "    y_ticks = [0.5, mean_acc, 1.0]\n",
    "\n",
    "    chart = alt.Chart(df).mark_bar(color=moral_colors[target]).encode(\n",
    "        x=alt.X('Annotator:O', axis=alt.Axis(labels=False)),\n",
    "        y=alt.Y('Accuracy:Q', axis=alt.Axis(values=y_ticks, format='.0%'))\n",
    "    )\n",
    "    chart = chart + alt.Chart(df.iloc[[ai_pos]]).mark_rule(\n",
    "        color='black',  strokeDash=[2,2]\n",
    "    ).encode(x='Annotator:O')\n",
    "    chart = chart + alt.Chart(df.iloc[[ai_pos]]).mark_text(\n",
    "        text='💻', fontSize=20, dy=-30, \n",
    "    ).encode(x='Annotator:O', y='Accuracy:Q')\n",
    "    chart = chart + alt.Chart(df.iloc[[ai_pos]]).mark_text(\n",
    "        text=f'{ai_percentile:.0f}th', \n",
    "        fontSize=12, \n",
    "        fontWeight='bold',\n",
    "        dy=-10\n",
    "    ).encode(x='Annotator:O', y='Accuracy:Q')\n",
    "    chart = chart + alt.Chart().mark_rule(\n",
    "        color=moral_colors[target], strokeDash=[2,2], strokeWidth=2,\n",
    "    ).encode(y=alt.datum(mean_acc))\n",
    "    chart = chart.properties(width=200, height=150)\n",
    "    chart = chart.resolve_legend(color='shared')\n",
    "    # chart = chart.facet(data=df, column='moral:N')\n",
    "\n",
    "    # chart = chart.facet(data=df,column='moral:N')\n",
    "    chart.show()\n",
    "    charts.append( chart )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_prior = tfp.distributions.Dirichlet([10,10],name='pi_prior')\n",
    "optimizer = tf.optimizers.Adam(1e-2, )\n",
    "\n",
    "@tf.function()\n",
    "def train_step(pi_logits, theta_logits, annot_ids):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = -log_p(pi_logits, theta_logits, annot_ids)\n",
    "    gradients = tape.gradient(loss, [pi_logits, theta_logits])\n",
    "    optimizer.apply_gradients(zip(gradients, [pi_logits, theta_logits]))\n",
    "    return loss\n",
    "\n",
    "device = \"/GPU:0\"\n",
    "with tf.device(device):\n",
    "    for _ in tqdm(range(3000), total=3000):\n",
    "        train_step(pi_logits, theta_logits, annot_ids, )    \n",
    "\n",
    "pi = tf.nn.softmax(pi_logits)\n",
    "theta = tf.nn.softmax(theta_logits, axis=-1)\n",
    "theta[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.init(project='morality-llm', id=run_id, resume=\"must\")\n",
    "wandb.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_scale = alt.Scale(\n",
    "   domain=list(moral_colors.keys()),\n",
    "   range=list(moral_colors.values())\n",
    ")\n",
    "\n",
    "for i, chart in enumerate(charts):\n",
    "   legend = alt.Legend(orient='bottom') if i == 0 else None\n",
    "   charts[i] = chart.encode(\n",
    "       color=alt.Color('Moral Dimension:N', scale=color_scale, legend=legend)\n",
    "   )\n",
    "\n",
    "grid = alt.vconcat(\n",
    "   alt.hconcat(charts[0], charts[1],),\n",
    "   alt.hconcat(charts[2], charts[3],),\n",
    "   alt.hconcat(charts[4], charts[5] )\n",
    ").resolve_scale(y='shared'\n",
    ").resolve_axis(y='shared'\n",
    ").resolve_legend(color='shared')\n",
    "\n",
    "grid = grid.properties(\n",
    "    title=alt.TitleParams(\n",
    "        text=[\"Human vs AI Performance \\n -- Mean Accuracy, -- 💻 AI System\"],\n",
    "        fontSize=14\n",
    "    )\n",
    ")\n",
    "\n",
    "grid.save(\"figures/accuracy.png\",ppi=300)\n",
    "grid.save(\"figures/accuracy.svg\")\n",
    "# wandb.summary.update({\"accuracy\": wandb.Image(\"figures/accuracy.png\")})\n",
    "# wandb.run.summary[\"accuracy\"] = wandb.Image(\"figures/accuracy.png\")\n",
    "wandb.log({\"accuracy\": wandb.Image(\"figures/accuracy.png\")})\n",
    "wandb.log_artifact(\"figures/accuracy.svg\", name=\"accuracy\", type=\"plot\")\n",
    "wandb.log({\"stats\": wandb.Table(dataframe=pd.DataFrame(results).T.reset_index())})\n",
    "\n",
    "grid.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.save(\"figures/accuracy.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "run = wandb.init()\n",
    "artifact = run.use_artifact('mskorski-university-of-warsaw/morality-llm/accuracy:v0', type='plot')\n",
    "artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarization\n",
    "\n",
    "Aggregate results from across multiple experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_ids = ['yon5adbv','2fisp7sj','pckcakff'] # MFTC\n",
    "run_ids = ['jv4exac1','d1kvjg6q','7krxewfl'] # eMFD\n",
    "# run_ids = ['jpsu9gfg','744jcvse','fq9kn2ok'] # MFRC\n",
    "\n",
    "df = []\n",
    "\n",
    "for run_id in tqdm(run_ids):\n",
    "\n",
    "    run = api.run(f\"{project_name}/{run_id}\")\n",
    "    artifact = api.artifact(f\"{project_name}/run-{run_id}-stats:latest\")\n",
    "    table = artifact.get(\"stats\").get_dataframe()\n",
    "    table['model_name'] = run.config['model_name']\n",
    "    table['data'] = run.config['test_data']\n",
    "    df.append( table )\n",
    "    \n",
    "df = pd.concat(df)\n",
    "df.rename({'index':'foundation'},axis=1,inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot = df.pivot_table(\n",
    "   index='foundation',\n",
    "   columns='model_name', \n",
    "   values=['ba_ai_rank', 'ba_ai', ],\n",
    "   aggfunc='first'\n",
    ")\n",
    "\n",
    "pivot.columns = pivot.columns.set_levels(['Acc%', 'Pct'], level=0)\n",
    "pivot.loc[:, ('Acc%', slice(None))] = pivot.loc[:, ('Acc%', slice(None))] * 100\n",
    "pivot = pivot.swaplevel(0, 1, axis=1)\n",
    "pivot = pivot.sort_index(axis=1, level=0)\n",
    "pivot = pivot.T\n",
    "pivot.index = pivot.index.set_names([\"Annotator\",\"Metric\"])\n",
    "pivot\n",
    "\n",
    "row = df.groupby(\"foundation\")[\"ba_mean\"].mean()*100\n",
    "pivot.loc[('Human','Avg Acc%'),:,] = row\n",
    "pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pivot.to_latex(float_format=\"%.0f\", escape=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      " 11%|█         | 1/9 [00:01<00:12,  1.52s/it]\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      " 22%|██▏       | 2/9 [00:03<00:10,  1.51s/it]\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      " 33%|███▎      | 3/9 [00:04<00:09,  1.51s/it]\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      " 44%|████▍     | 4/9 [00:06<00:07,  1.55s/it]\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      " 56%|█████▌    | 5/9 [00:07<00:06,  1.54s/it]\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      " 67%|██████▋   | 6/9 [00:09<00:04,  1.52s/it]\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      " 78%|███████▊  | 7/9 [00:10<00:03,  1.55s/it]\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      " 89%|████████▉ | 8/9 [00:12<00:01,  1.52s/it]\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "100%|██████████| 9/9 [00:13<00:00,  1.49s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>foundation</th>\n",
       "      <th>tpr_ai_rank</th>\n",
       "      <th>tnr_ai_rank</th>\n",
       "      <th>tpr_mean</th>\n",
       "      <th>tnr_mean</th>\n",
       "      <th>ba_mean</th>\n",
       "      <th>ba_ai_rank</th>\n",
       "      <th>tpr_ai</th>\n",
       "      <th>tnr_ai</th>\n",
       "      <th>ba_ai</th>\n",
       "      <th>pi_1</th>\n",
       "      <th>model_name</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>care</td>\n",
       "      <td>83.333333</td>\n",
       "      <td>4.166667</td>\n",
       "      <td>0.503838</td>\n",
       "      <td>0.932504</td>\n",
       "      <td>0.718171</td>\n",
       "      <td>70.833333</td>\n",
       "      <td>0.848118</td>\n",
       "      <td>0.715231</td>\n",
       "      <td>0.781675</td>\n",
       "      <td>0.283195</td>\n",
       "      <td>deepseek-v3</td>\n",
       "      <td>morality-MFTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fairness</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>29.166667</td>\n",
       "      <td>0.571687</td>\n",
       "      <td>0.936933</td>\n",
       "      <td>0.754310</td>\n",
       "      <td>70.833333</td>\n",
       "      <td>0.713705</td>\n",
       "      <td>0.913012</td>\n",
       "      <td>0.813358</td>\n",
       "      <td>0.221594</td>\n",
       "      <td>deepseek-v3</td>\n",
       "      <td>morality-MFTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>loyalty</td>\n",
       "      <td>83.333333</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>0.534841</td>\n",
       "      <td>0.914832</td>\n",
       "      <td>0.724837</td>\n",
       "      <td>79.166667</td>\n",
       "      <td>0.771089</td>\n",
       "      <td>0.832729</td>\n",
       "      <td>0.801909</td>\n",
       "      <td>0.115926</td>\n",
       "      <td>deepseek-v3</td>\n",
       "      <td>morality-MFTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>authority</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>29.166667</td>\n",
       "      <td>0.468762</td>\n",
       "      <td>0.867069</td>\n",
       "      <td>0.667915</td>\n",
       "      <td>87.500000</td>\n",
       "      <td>0.753710</td>\n",
       "      <td>0.889110</td>\n",
       "      <td>0.821410</td>\n",
       "      <td>0.187448</td>\n",
       "      <td>deepseek-v3</td>\n",
       "      <td>morality-MFTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sanctity</td>\n",
       "      <td>83.333333</td>\n",
       "      <td>41.666667</td>\n",
       "      <td>0.420978</td>\n",
       "      <td>0.926920</td>\n",
       "      <td>0.673949</td>\n",
       "      <td>91.666667</td>\n",
       "      <td>0.732782</td>\n",
       "      <td>0.962425</td>\n",
       "      <td>0.847604</td>\n",
       "      <td>0.148356</td>\n",
       "      <td>deepseek-v3</td>\n",
       "      <td>morality-MFTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>any</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>29.166667</td>\n",
       "      <td>0.732798</td>\n",
       "      <td>0.715026</td>\n",
       "      <td>0.723912</td>\n",
       "      <td>54.166667</td>\n",
       "      <td>0.973736</td>\n",
       "      <td>0.565991</td>\n",
       "      <td>0.769863</td>\n",
       "      <td>0.636540</td>\n",
       "      <td>deepseek-v3</td>\n",
       "      <td>morality-MFTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>care</td>\n",
       "      <td>83.333333</td>\n",
       "      <td>4.166667</td>\n",
       "      <td>0.507586</td>\n",
       "      <td>0.931878</td>\n",
       "      <td>0.719732</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>0.800954</td>\n",
       "      <td>0.718333</td>\n",
       "      <td>0.759643</td>\n",
       "      <td>0.277201</td>\n",
       "      <td>llama4_maverick</td>\n",
       "      <td>morality-MFTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fairness</td>\n",
       "      <td>79.166667</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>0.557706</td>\n",
       "      <td>0.939610</td>\n",
       "      <td>0.748658</td>\n",
       "      <td>70.833333</td>\n",
       "      <td>0.771988</td>\n",
       "      <td>0.885044</td>\n",
       "      <td>0.828516</td>\n",
       "      <td>0.228853</td>\n",
       "      <td>llama4_maverick</td>\n",
       "      <td>morality-MFTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>loyalty</td>\n",
       "      <td>95.833333</td>\n",
       "      <td>8.333333</td>\n",
       "      <td>0.530836</td>\n",
       "      <td>0.910353</td>\n",
       "      <td>0.720595</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>0.867138</td>\n",
       "      <td>0.722938</td>\n",
       "      <td>0.795038</td>\n",
       "      <td>0.118964</td>\n",
       "      <td>llama4_maverick</td>\n",
       "      <td>morality-MFTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>authority</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>29.166667</td>\n",
       "      <td>0.463266</td>\n",
       "      <td>0.867558</td>\n",
       "      <td>0.665412</td>\n",
       "      <td>79.166667</td>\n",
       "      <td>0.761056</td>\n",
       "      <td>0.820440</td>\n",
       "      <td>0.790748</td>\n",
       "      <td>0.189762</td>\n",
       "      <td>llama4_maverick</td>\n",
       "      <td>morality-MFTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sanctity</td>\n",
       "      <td>91.666667</td>\n",
       "      <td>20.833333</td>\n",
       "      <td>0.401350</td>\n",
       "      <td>0.927418</td>\n",
       "      <td>0.664384</td>\n",
       "      <td>95.833333</td>\n",
       "      <td>0.838944</td>\n",
       "      <td>0.907729</td>\n",
       "      <td>0.873337</td>\n",
       "      <td>0.185306</td>\n",
       "      <td>llama4_maverick</td>\n",
       "      <td>morality-MFTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>any</td>\n",
       "      <td>83.333333</td>\n",
       "      <td>20.833333</td>\n",
       "      <td>0.734670</td>\n",
       "      <td>0.709002</td>\n",
       "      <td>0.721836</td>\n",
       "      <td>41.666667</td>\n",
       "      <td>0.995034</td>\n",
       "      <td>0.369557</td>\n",
       "      <td>0.682296</td>\n",
       "      <td>0.659256</td>\n",
       "      <td>llama4_maverick</td>\n",
       "      <td>morality-MFTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>care</td>\n",
       "      <td>95.833333</td>\n",
       "      <td>4.166667</td>\n",
       "      <td>0.479800</td>\n",
       "      <td>0.929145</td>\n",
       "      <td>0.704472</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>0.927889</td>\n",
       "      <td>0.641957</td>\n",
       "      <td>0.784923</td>\n",
       "      <td>0.302884</td>\n",
       "      <td>claude-4-sonnet</td>\n",
       "      <td>morality-MFTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fairness</td>\n",
       "      <td>95.833333</td>\n",
       "      <td>4.166667</td>\n",
       "      <td>0.564137</td>\n",
       "      <td>0.936411</td>\n",
       "      <td>0.750274</td>\n",
       "      <td>70.833333</td>\n",
       "      <td>0.906712</td>\n",
       "      <td>0.741816</td>\n",
       "      <td>0.824264</td>\n",
       "      <td>0.237133</td>\n",
       "      <td>claude-4-sonnet</td>\n",
       "      <td>morality-MFTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>loyalty</td>\n",
       "      <td>83.333333</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>0.526718</td>\n",
       "      <td>0.915917</td>\n",
       "      <td>0.721318</td>\n",
       "      <td>79.166667</td>\n",
       "      <td>0.798585</td>\n",
       "      <td>0.812953</td>\n",
       "      <td>0.805769</td>\n",
       "      <td>0.121125</td>\n",
       "      <td>claude-4-sonnet</td>\n",
       "      <td>morality-MFTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>authority</td>\n",
       "      <td>87.500000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.476302</td>\n",
       "      <td>0.864747</td>\n",
       "      <td>0.670525</td>\n",
       "      <td>83.333333</td>\n",
       "      <td>0.852820</td>\n",
       "      <td>0.747160</td>\n",
       "      <td>0.799990</td>\n",
       "      <td>0.191893</td>\n",
       "      <td>claude-4-sonnet</td>\n",
       "      <td>morality-MFTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sanctity</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>20.833333</td>\n",
       "      <td>0.405719</td>\n",
       "      <td>0.925887</td>\n",
       "      <td>0.665803</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.921939</td>\n",
       "      <td>0.878151</td>\n",
       "      <td>0.900045</td>\n",
       "      <td>0.185016</td>\n",
       "      <td>claude-4-sonnet</td>\n",
       "      <td>morality-MFTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>any</td>\n",
       "      <td>83.333333</td>\n",
       "      <td>29.166667</td>\n",
       "      <td>0.735349</td>\n",
       "      <td>0.717566</td>\n",
       "      <td>0.726458</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.991297</td>\n",
       "      <td>0.511685</td>\n",
       "      <td>0.751491</td>\n",
       "      <td>0.657149</td>\n",
       "      <td>claude-4-sonnet</td>\n",
       "      <td>morality-MFTC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>care</td>\n",
       "      <td>97.099237</td>\n",
       "      <td>46.259542</td>\n",
       "      <td>0.398663</td>\n",
       "      <td>0.847360</td>\n",
       "      <td>0.623011</td>\n",
       "      <td>97.557252</td>\n",
       "      <td>0.920779</td>\n",
       "      <td>0.859454</td>\n",
       "      <td>0.890116</td>\n",
       "      <td>0.261139</td>\n",
       "      <td>llama4_maverick</td>\n",
       "      <td>morality-eMFD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fairness</td>\n",
       "      <td>87.175573</td>\n",
       "      <td>52.061069</td>\n",
       "      <td>0.458401</td>\n",
       "      <td>0.838975</td>\n",
       "      <td>0.648688</td>\n",
       "      <td>88.091603</td>\n",
       "      <td>0.818491</td>\n",
       "      <td>0.866037</td>\n",
       "      <td>0.842264</td>\n",
       "      <td>0.183752</td>\n",
       "      <td>llama4_maverick</td>\n",
       "      <td>morality-eMFD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>loyalty</td>\n",
       "      <td>90.839695</td>\n",
       "      <td>46.564885</td>\n",
       "      <td>0.414793</td>\n",
       "      <td>0.837054</td>\n",
       "      <td>0.625923</td>\n",
       "      <td>91.450382</td>\n",
       "      <td>0.800634</td>\n",
       "      <td>0.852023</td>\n",
       "      <td>0.826329</td>\n",
       "      <td>0.195337</td>\n",
       "      <td>llama4_maverick</td>\n",
       "      <td>morality-eMFD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>authority</td>\n",
       "      <td>94.809160</td>\n",
       "      <td>28.244275</td>\n",
       "      <td>0.431197</td>\n",
       "      <td>0.842912</td>\n",
       "      <td>0.637055</td>\n",
       "      <td>92.366412</td>\n",
       "      <td>0.874307</td>\n",
       "      <td>0.792928</td>\n",
       "      <td>0.833618</td>\n",
       "      <td>0.237539</td>\n",
       "      <td>llama4_maverick</td>\n",
       "      <td>morality-eMFD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sanctity</td>\n",
       "      <td>80.763359</td>\n",
       "      <td>79.847328</td>\n",
       "      <td>0.382706</td>\n",
       "      <td>0.863556</td>\n",
       "      <td>0.623131</td>\n",
       "      <td>85.954198</td>\n",
       "      <td>0.677540</td>\n",
       "      <td>0.962066</td>\n",
       "      <td>0.819803</td>\n",
       "      <td>0.102525</td>\n",
       "      <td>llama4_maverick</td>\n",
       "      <td>morality-eMFD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>any</td>\n",
       "      <td>0.458015</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.994967</td>\n",
       "      <td>0.256158</td>\n",
       "      <td>0.625563</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.911402</td>\n",
       "      <td>0.989338</td>\n",
       "      <td>0.950370</td>\n",
       "      <td>0.927654</td>\n",
       "      <td>llama4_maverick</td>\n",
       "      <td>morality-eMFD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>care</td>\n",
       "      <td>96.030534</td>\n",
       "      <td>40.610687</td>\n",
       "      <td>0.402625</td>\n",
       "      <td>0.847604</td>\n",
       "      <td>0.625115</td>\n",
       "      <td>96.793893</td>\n",
       "      <td>0.908832</td>\n",
       "      <td>0.847756</td>\n",
       "      <td>0.878294</td>\n",
       "      <td>0.260492</td>\n",
       "      <td>claude-4-sonnet</td>\n",
       "      <td>morality-eMFD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fairness</td>\n",
       "      <td>90.076336</td>\n",
       "      <td>47.022901</td>\n",
       "      <td>0.429838</td>\n",
       "      <td>0.836939</td>\n",
       "      <td>0.633388</td>\n",
       "      <td>90.992366</td>\n",
       "      <td>0.807292</td>\n",
       "      <td>0.849649</td>\n",
       "      <td>0.828470</td>\n",
       "      <td>0.184133</td>\n",
       "      <td>claude-4-sonnet</td>\n",
       "      <td>morality-eMFD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>loyalty</td>\n",
       "      <td>75.114504</td>\n",
       "      <td>57.099237</td>\n",
       "      <td>0.391968</td>\n",
       "      <td>0.837007</td>\n",
       "      <td>0.614487</td>\n",
       "      <td>78.473282</td>\n",
       "      <td>0.551092</td>\n",
       "      <td>0.894105</td>\n",
       "      <td>0.722598</td>\n",
       "      <td>0.214984</td>\n",
       "      <td>claude-4-sonnet</td>\n",
       "      <td>morality-eMFD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>authority</td>\n",
       "      <td>76.488550</td>\n",
       "      <td>37.557252</td>\n",
       "      <td>0.435386</td>\n",
       "      <td>0.836723</td>\n",
       "      <td>0.636054</td>\n",
       "      <td>74.961832</td>\n",
       "      <td>0.663505</td>\n",
       "      <td>0.815985</td>\n",
       "      <td>0.739745</td>\n",
       "      <td>0.212498</td>\n",
       "      <td>claude-4-sonnet</td>\n",
       "      <td>morality-eMFD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sanctity</td>\n",
       "      <td>77.557252</td>\n",
       "      <td>76.030534</td>\n",
       "      <td>0.396159</td>\n",
       "      <td>0.866528</td>\n",
       "      <td>0.631343</td>\n",
       "      <td>83.511450</td>\n",
       "      <td>0.638513</td>\n",
       "      <td>0.958979</td>\n",
       "      <td>0.798746</td>\n",
       "      <td>0.104489</td>\n",
       "      <td>claude-4-sonnet</td>\n",
       "      <td>morality-eMFD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>any</td>\n",
       "      <td>0.152672</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.994767</td>\n",
       "      <td>0.266030</td>\n",
       "      <td>0.630398</td>\n",
       "      <td>94.809160</td>\n",
       "      <td>0.705935</td>\n",
       "      <td>0.998814</td>\n",
       "      <td>0.852374</td>\n",
       "      <td>0.939836</td>\n",
       "      <td>claude-4-sonnet</td>\n",
       "      <td>morality-eMFD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>care</td>\n",
       "      <td>97.099237</td>\n",
       "      <td>38.320611</td>\n",
       "      <td>0.404248</td>\n",
       "      <td>0.847733</td>\n",
       "      <td>0.625990</td>\n",
       "      <td>97.557252</td>\n",
       "      <td>0.924815</td>\n",
       "      <td>0.839696</td>\n",
       "      <td>0.882255</td>\n",
       "      <td>0.267188</td>\n",
       "      <td>deepseek-v3</td>\n",
       "      <td>morality-eMFD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fairness</td>\n",
       "      <td>79.541985</td>\n",
       "      <td>58.931298</td>\n",
       "      <td>0.465095</td>\n",
       "      <td>0.837305</td>\n",
       "      <td>0.651200</td>\n",
       "      <td>85.190840</td>\n",
       "      <td>0.750436</td>\n",
       "      <td>0.884533</td>\n",
       "      <td>0.817484</td>\n",
       "      <td>0.172338</td>\n",
       "      <td>deepseek-v3</td>\n",
       "      <td>morality-eMFD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>loyalty</td>\n",
       "      <td>92.519084</td>\n",
       "      <td>54.656489</td>\n",
       "      <td>0.410887</td>\n",
       "      <td>0.834146</td>\n",
       "      <td>0.622516</td>\n",
       "      <td>93.282443</td>\n",
       "      <td>0.831875</td>\n",
       "      <td>0.870304</td>\n",
       "      <td>0.851090</td>\n",
       "      <td>0.187462</td>\n",
       "      <td>deepseek-v3</td>\n",
       "      <td>morality-eMFD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>authority</td>\n",
       "      <td>90.839695</td>\n",
       "      <td>32.519084</td>\n",
       "      <td>0.438328</td>\n",
       "      <td>0.841080</td>\n",
       "      <td>0.639704</td>\n",
       "      <td>89.160305</td>\n",
       "      <td>0.817773</td>\n",
       "      <td>0.810028</td>\n",
       "      <td>0.813901</td>\n",
       "      <td>0.226880</td>\n",
       "      <td>deepseek-v3</td>\n",
       "      <td>morality-eMFD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sanctity</td>\n",
       "      <td>69.465649</td>\n",
       "      <td>84.274809</td>\n",
       "      <td>0.385468</td>\n",
       "      <td>0.863901</td>\n",
       "      <td>0.624684</td>\n",
       "      <td>76.946565</td>\n",
       "      <td>0.522351</td>\n",
       "      <td>0.975883</td>\n",
       "      <td>0.749117</td>\n",
       "      <td>0.100765</td>\n",
       "      <td>deepseek-v3</td>\n",
       "      <td>morality-eMFD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>any</td>\n",
       "      <td>0.152672</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.994949</td>\n",
       "      <td>0.267865</td>\n",
       "      <td>0.631407</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.862875</td>\n",
       "      <td>0.992899</td>\n",
       "      <td>0.927887</td>\n",
       "      <td>0.930401</td>\n",
       "      <td>deepseek-v3</td>\n",
       "      <td>morality-eMFD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>care</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>14.285714</td>\n",
       "      <td>0.582710</td>\n",
       "      <td>0.945589</td>\n",
       "      <td>0.764149</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.927149</td>\n",
       "      <td>0.862936</td>\n",
       "      <td>0.895043</td>\n",
       "      <td>0.181039</td>\n",
       "      <td>deepseek-v3</td>\n",
       "      <td>morality-MFRC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fairness</td>\n",
       "      <td>71.428571</td>\n",
       "      <td>28.571429</td>\n",
       "      <td>0.586923</td>\n",
       "      <td>0.913795</td>\n",
       "      <td>0.750359</td>\n",
       "      <td>42.857143</td>\n",
       "      <td>0.635849</td>\n",
       "      <td>0.820359</td>\n",
       "      <td>0.728104</td>\n",
       "      <td>0.154020</td>\n",
       "      <td>deepseek-v3</td>\n",
       "      <td>morality-MFRC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>loyalty</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>14.285714</td>\n",
       "      <td>0.482934</td>\n",
       "      <td>0.945514</td>\n",
       "      <td>0.714224</td>\n",
       "      <td>85.714286</td>\n",
       "      <td>0.903763</td>\n",
       "      <td>0.785650</td>\n",
       "      <td>0.844706</td>\n",
       "      <td>0.090721</td>\n",
       "      <td>deepseek-v3</td>\n",
       "      <td>morality-MFRC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>authority</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>14.285714</td>\n",
       "      <td>0.441830</td>\n",
       "      <td>0.952307</td>\n",
       "      <td>0.697069</td>\n",
       "      <td>85.714286</td>\n",
       "      <td>0.812809</td>\n",
       "      <td>0.856368</td>\n",
       "      <td>0.834588</td>\n",
       "      <td>0.165325</td>\n",
       "      <td>deepseek-v3</td>\n",
       "      <td>morality-MFRC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sanctity</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>42.857143</td>\n",
       "      <td>0.452291</td>\n",
       "      <td>0.967405</td>\n",
       "      <td>0.709848</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.687266</td>\n",
       "      <td>0.955350</td>\n",
       "      <td>0.821308</td>\n",
       "      <td>0.042951</td>\n",
       "      <td>deepseek-v3</td>\n",
       "      <td>morality-MFRC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>any</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>14.285714</td>\n",
       "      <td>0.687301</td>\n",
       "      <td>0.818768</td>\n",
       "      <td>0.753035</td>\n",
       "      <td>14.285714</td>\n",
       "      <td>0.971469</td>\n",
       "      <td>0.389927</td>\n",
       "      <td>0.680698</td>\n",
       "      <td>0.454398</td>\n",
       "      <td>deepseek-v3</td>\n",
       "      <td>morality-MFRC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>care</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>14.285714</td>\n",
       "      <td>0.562866</td>\n",
       "      <td>0.947595</td>\n",
       "      <td>0.755230</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.947141</td>\n",
       "      <td>0.847716</td>\n",
       "      <td>0.897429</td>\n",
       "      <td>0.201207</td>\n",
       "      <td>claude-4-sonnet</td>\n",
       "      <td>morality-MFRC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fairness</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>14.285714</td>\n",
       "      <td>0.597956</td>\n",
       "      <td>0.905402</td>\n",
       "      <td>0.751679</td>\n",
       "      <td>85.714286</td>\n",
       "      <td>0.877366</td>\n",
       "      <td>0.728353</td>\n",
       "      <td>0.802860</td>\n",
       "      <td>0.172546</td>\n",
       "      <td>claude-4-sonnet</td>\n",
       "      <td>morality-MFRC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>loyalty</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>14.285714</td>\n",
       "      <td>0.475300</td>\n",
       "      <td>0.950501</td>\n",
       "      <td>0.712901</td>\n",
       "      <td>85.714286</td>\n",
       "      <td>0.912768</td>\n",
       "      <td>0.804819</td>\n",
       "      <td>0.858793</td>\n",
       "      <td>0.096324</td>\n",
       "      <td>claude-4-sonnet</td>\n",
       "      <td>morality-MFRC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>authority</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>14.285714</td>\n",
       "      <td>0.422069</td>\n",
       "      <td>0.947511</td>\n",
       "      <td>0.684790</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.834755</td>\n",
       "      <td>0.827764</td>\n",
       "      <td>0.831259</td>\n",
       "      <td>0.165226</td>\n",
       "      <td>claude-4-sonnet</td>\n",
       "      <td>morality-MFRC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sanctity</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>14.285714</td>\n",
       "      <td>0.444998</td>\n",
       "      <td>0.961762</td>\n",
       "      <td>0.703380</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.919810</td>\n",
       "      <td>0.905341</td>\n",
       "      <td>0.912575</td>\n",
       "      <td>0.056390</td>\n",
       "      <td>claude-4-sonnet</td>\n",
       "      <td>morality-MFRC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>any</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>14.285714</td>\n",
       "      <td>0.687942</td>\n",
       "      <td>0.832111</td>\n",
       "      <td>0.760026</td>\n",
       "      <td>57.142857</td>\n",
       "      <td>0.984973</td>\n",
       "      <td>0.486614</td>\n",
       "      <td>0.735793</td>\n",
       "      <td>0.453823</td>\n",
       "      <td>claude-4-sonnet</td>\n",
       "      <td>morality-MFRC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>care</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>28.571429</td>\n",
       "      <td>0.582344</td>\n",
       "      <td>0.947670</td>\n",
       "      <td>0.765007</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.892366</td>\n",
       "      <td>0.885767</td>\n",
       "      <td>0.889067</td>\n",
       "      <td>0.177754</td>\n",
       "      <td>llama4_maverick</td>\n",
       "      <td>morality-MFRC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fairness</td>\n",
       "      <td>71.428571</td>\n",
       "      <td>14.285714</td>\n",
       "      <td>0.589254</td>\n",
       "      <td>0.900268</td>\n",
       "      <td>0.744761</td>\n",
       "      <td>42.857143</td>\n",
       "      <td>0.718883</td>\n",
       "      <td>0.712374</td>\n",
       "      <td>0.715629</td>\n",
       "      <td>0.163557</td>\n",
       "      <td>llama4_maverick</td>\n",
       "      <td>morality-MFRC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>loyalty</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>14.285714</td>\n",
       "      <td>0.469500</td>\n",
       "      <td>0.942381</td>\n",
       "      <td>0.705940</td>\n",
       "      <td>85.714286</td>\n",
       "      <td>0.902690</td>\n",
       "      <td>0.751576</td>\n",
       "      <td>0.827133</td>\n",
       "      <td>0.099224</td>\n",
       "      <td>llama4_maverick</td>\n",
       "      <td>morality-MFRC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>authority</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>14.285714</td>\n",
       "      <td>0.441156</td>\n",
       "      <td>0.942016</td>\n",
       "      <td>0.691586</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.854024</td>\n",
       "      <td>0.800037</td>\n",
       "      <td>0.827031</td>\n",
       "      <td>0.156141</td>\n",
       "      <td>llama4_maverick</td>\n",
       "      <td>morality-MFRC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sanctity</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>14.285714</td>\n",
       "      <td>0.440538</td>\n",
       "      <td>0.959887</td>\n",
       "      <td>0.700213</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>0.847981</td>\n",
       "      <td>0.895059</td>\n",
       "      <td>0.871520</td>\n",
       "      <td>0.051482</td>\n",
       "      <td>llama4_maverick</td>\n",
       "      <td>morality-MFRC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>any</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>14.285714</td>\n",
       "      <td>0.691660</td>\n",
       "      <td>0.795834</td>\n",
       "      <td>0.743747</td>\n",
       "      <td>14.285714</td>\n",
       "      <td>0.990831</td>\n",
       "      <td>0.250089</td>\n",
       "      <td>0.620460</td>\n",
       "      <td>0.451020</td>\n",
       "      <td>llama4_maverick</td>\n",
       "      <td>morality-MFRC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  foundation  tpr_ai_rank  tnr_ai_rank  tpr_mean  tnr_mean   ba_mean  \\\n",
       "0       care    83.333333     4.166667  0.503838  0.932504  0.718171   \n",
       "1   fairness    66.666667    29.166667  0.571687  0.936933  0.754310   \n",
       "2    loyalty    83.333333    12.500000  0.534841  0.914832  0.724837   \n",
       "3  authority    75.000000    29.166667  0.468762  0.867069  0.667915   \n",
       "4   sanctity    83.333333    41.666667  0.420978  0.926920  0.673949   \n",
       "5        any    75.000000    29.166667  0.732798  0.715026  0.723912   \n",
       "0       care    83.333333     4.166667  0.507586  0.931878  0.719732   \n",
       "1   fairness    79.166667    12.500000  0.557706  0.939610  0.748658   \n",
       "2    loyalty    95.833333     8.333333  0.530836  0.910353  0.720595   \n",
       "3  authority    75.000000    29.166667  0.463266  0.867558  0.665412   \n",
       "4   sanctity    91.666667    20.833333  0.401350  0.927418  0.664384   \n",
       "5        any    83.333333    20.833333  0.734670  0.709002  0.721836   \n",
       "0       care    95.833333     4.166667  0.479800  0.929145  0.704472   \n",
       "1   fairness    95.833333     4.166667  0.564137  0.936411  0.750274   \n",
       "2    loyalty    83.333333    12.500000  0.526718  0.915917  0.721318   \n",
       "3  authority    87.500000    25.000000  0.476302  0.864747  0.670525   \n",
       "4   sanctity   100.000000    20.833333  0.405719  0.925887  0.665803   \n",
       "5        any    83.333333    29.166667  0.735349  0.717566  0.726458   \n",
       "0       care    97.099237    46.259542  0.398663  0.847360  0.623011   \n",
       "1   fairness    87.175573    52.061069  0.458401  0.838975  0.648688   \n",
       "2    loyalty    90.839695    46.564885  0.414793  0.837054  0.625923   \n",
       "3  authority    94.809160    28.244275  0.431197  0.842912  0.637055   \n",
       "4   sanctity    80.763359    79.847328  0.382706  0.863556  0.623131   \n",
       "5        any     0.458015   100.000000  0.994967  0.256158  0.625563   \n",
       "0       care    96.030534    40.610687  0.402625  0.847604  0.625115   \n",
       "1   fairness    90.076336    47.022901  0.429838  0.836939  0.633388   \n",
       "2    loyalty    75.114504    57.099237  0.391968  0.837007  0.614487   \n",
       "3  authority    76.488550    37.557252  0.435386  0.836723  0.636054   \n",
       "4   sanctity    77.557252    76.030534  0.396159  0.866528  0.631343   \n",
       "5        any     0.152672   100.000000  0.994767  0.266030  0.630398   \n",
       "0       care    97.099237    38.320611  0.404248  0.847733  0.625990   \n",
       "1   fairness    79.541985    58.931298  0.465095  0.837305  0.651200   \n",
       "2    loyalty    92.519084    54.656489  0.410887  0.834146  0.622516   \n",
       "3  authority    90.839695    32.519084  0.438328  0.841080  0.639704   \n",
       "4   sanctity    69.465649    84.274809  0.385468  0.863901  0.624684   \n",
       "5        any     0.152672   100.000000  0.994949  0.267865  0.631407   \n",
       "0       care   100.000000    14.285714  0.582710  0.945589  0.764149   \n",
       "1   fairness    71.428571    28.571429  0.586923  0.913795  0.750359   \n",
       "2    loyalty   100.000000    14.285714  0.482934  0.945514  0.714224   \n",
       "3  authority   100.000000    14.285714  0.441830  0.952307  0.697069   \n",
       "4   sanctity   100.000000    42.857143  0.452291  0.967405  0.709848   \n",
       "5        any   100.000000    14.285714  0.687301  0.818768  0.753035   \n",
       "0       care   100.000000    14.285714  0.562866  0.947595  0.755230   \n",
       "1   fairness   100.000000    14.285714  0.597956  0.905402  0.751679   \n",
       "2    loyalty   100.000000    14.285714  0.475300  0.950501  0.712901   \n",
       "3  authority   100.000000    14.285714  0.422069  0.947511  0.684790   \n",
       "4   sanctity   100.000000    14.285714  0.444998  0.961762  0.703380   \n",
       "5        any   100.000000    14.285714  0.687942  0.832111  0.760026   \n",
       "0       care   100.000000    28.571429  0.582344  0.947670  0.765007   \n",
       "1   fairness    71.428571    14.285714  0.589254  0.900268  0.744761   \n",
       "2    loyalty   100.000000    14.285714  0.469500  0.942381  0.705940   \n",
       "3  authority   100.000000    14.285714  0.441156  0.942016  0.691586   \n",
       "4   sanctity   100.000000    14.285714  0.440538  0.959887  0.700213   \n",
       "5        any   100.000000    14.285714  0.691660  0.795834  0.743747   \n",
       "\n",
       "   ba_ai_rank    tpr_ai    tnr_ai     ba_ai      pi_1       model_name  \\\n",
       "0   70.833333  0.848118  0.715231  0.781675  0.283195      deepseek-v3   \n",
       "1   70.833333  0.713705  0.913012  0.813358  0.221594      deepseek-v3   \n",
       "2   79.166667  0.771089  0.832729  0.801909  0.115926      deepseek-v3   \n",
       "3   87.500000  0.753710  0.889110  0.821410  0.187448      deepseek-v3   \n",
       "4   91.666667  0.732782  0.962425  0.847604  0.148356      deepseek-v3   \n",
       "5   54.166667  0.973736  0.565991  0.769863  0.636540      deepseek-v3   \n",
       "0   62.500000  0.800954  0.718333  0.759643  0.277201  llama4_maverick   \n",
       "1   70.833333  0.771988  0.885044  0.828516  0.228853  llama4_maverick   \n",
       "2   75.000000  0.867138  0.722938  0.795038  0.118964  llama4_maverick   \n",
       "3   79.166667  0.761056  0.820440  0.790748  0.189762  llama4_maverick   \n",
       "4   95.833333  0.838944  0.907729  0.873337  0.185306  llama4_maverick   \n",
       "5   41.666667  0.995034  0.369557  0.682296  0.659256  llama4_maverick   \n",
       "0   75.000000  0.927889  0.641957  0.784923  0.302884  claude-4-sonnet   \n",
       "1   70.833333  0.906712  0.741816  0.824264  0.237133  claude-4-sonnet   \n",
       "2   79.166667  0.798585  0.812953  0.805769  0.121125  claude-4-sonnet   \n",
       "3   83.333333  0.852820  0.747160  0.799990  0.191893  claude-4-sonnet   \n",
       "4  100.000000  0.921939  0.878151  0.900045  0.185016  claude-4-sonnet   \n",
       "5   50.000000  0.991297  0.511685  0.751491  0.657149  claude-4-sonnet   \n",
       "0   97.557252  0.920779  0.859454  0.890116  0.261139  llama4_maverick   \n",
       "1   88.091603  0.818491  0.866037  0.842264  0.183752  llama4_maverick   \n",
       "2   91.450382  0.800634  0.852023  0.826329  0.195337  llama4_maverick   \n",
       "3   92.366412  0.874307  0.792928  0.833618  0.237539  llama4_maverick   \n",
       "4   85.954198  0.677540  0.962066  0.819803  0.102525  llama4_maverick   \n",
       "5  100.000000  0.911402  0.989338  0.950370  0.927654  llama4_maverick   \n",
       "0   96.793893  0.908832  0.847756  0.878294  0.260492  claude-4-sonnet   \n",
       "1   90.992366  0.807292  0.849649  0.828470  0.184133  claude-4-sonnet   \n",
       "2   78.473282  0.551092  0.894105  0.722598  0.214984  claude-4-sonnet   \n",
       "3   74.961832  0.663505  0.815985  0.739745  0.212498  claude-4-sonnet   \n",
       "4   83.511450  0.638513  0.958979  0.798746  0.104489  claude-4-sonnet   \n",
       "5   94.809160  0.705935  0.998814  0.852374  0.939836  claude-4-sonnet   \n",
       "0   97.557252  0.924815  0.839696  0.882255  0.267188      deepseek-v3   \n",
       "1   85.190840  0.750436  0.884533  0.817484  0.172338      deepseek-v3   \n",
       "2   93.282443  0.831875  0.870304  0.851090  0.187462      deepseek-v3   \n",
       "3   89.160305  0.817773  0.810028  0.813901  0.226880      deepseek-v3   \n",
       "4   76.946565  0.522351  0.975883  0.749117  0.100765      deepseek-v3   \n",
       "5  100.000000  0.862875  0.992899  0.927887  0.930401      deepseek-v3   \n",
       "0  100.000000  0.927149  0.862936  0.895043  0.181039      deepseek-v3   \n",
       "1   42.857143  0.635849  0.820359  0.728104  0.154020      deepseek-v3   \n",
       "2   85.714286  0.903763  0.785650  0.844706  0.090721      deepseek-v3   \n",
       "3   85.714286  0.812809  0.856368  0.834588  0.165325      deepseek-v3   \n",
       "4  100.000000  0.687266  0.955350  0.821308  0.042951      deepseek-v3   \n",
       "5   14.285714  0.971469  0.389927  0.680698  0.454398      deepseek-v3   \n",
       "0  100.000000  0.947141  0.847716  0.897429  0.201207  claude-4-sonnet   \n",
       "1   85.714286  0.877366  0.728353  0.802860  0.172546  claude-4-sonnet   \n",
       "2   85.714286  0.912768  0.804819  0.858793  0.096324  claude-4-sonnet   \n",
       "3  100.000000  0.834755  0.827764  0.831259  0.165226  claude-4-sonnet   \n",
       "4  100.000000  0.919810  0.905341  0.912575  0.056390  claude-4-sonnet   \n",
       "5   57.142857  0.984973  0.486614  0.735793  0.453823  claude-4-sonnet   \n",
       "0  100.000000  0.892366  0.885767  0.889067  0.177754  llama4_maverick   \n",
       "1   42.857143  0.718883  0.712374  0.715629  0.163557  llama4_maverick   \n",
       "2   85.714286  0.902690  0.751576  0.827133  0.099224  llama4_maverick   \n",
       "3  100.000000  0.854024  0.800037  0.827031  0.156141  llama4_maverick   \n",
       "4  100.000000  0.847981  0.895059  0.871520  0.051482  llama4_maverick   \n",
       "5   14.285714  0.990831  0.250089  0.620460  0.451020  llama4_maverick   \n",
       "\n",
       "            data  \n",
       "0  morality-MFTC  \n",
       "1  morality-MFTC  \n",
       "2  morality-MFTC  \n",
       "3  morality-MFTC  \n",
       "4  morality-MFTC  \n",
       "5  morality-MFTC  \n",
       "0  morality-MFTC  \n",
       "1  morality-MFTC  \n",
       "2  morality-MFTC  \n",
       "3  morality-MFTC  \n",
       "4  morality-MFTC  \n",
       "5  morality-MFTC  \n",
       "0  morality-MFTC  \n",
       "1  morality-MFTC  \n",
       "2  morality-MFTC  \n",
       "3  morality-MFTC  \n",
       "4  morality-MFTC  \n",
       "5  morality-MFTC  \n",
       "0  morality-eMFD  \n",
       "1  morality-eMFD  \n",
       "2  morality-eMFD  \n",
       "3  morality-eMFD  \n",
       "4  morality-eMFD  \n",
       "5  morality-eMFD  \n",
       "0  morality-eMFD  \n",
       "1  morality-eMFD  \n",
       "2  morality-eMFD  \n",
       "3  morality-eMFD  \n",
       "4  morality-eMFD  \n",
       "5  morality-eMFD  \n",
       "0  morality-eMFD  \n",
       "1  morality-eMFD  \n",
       "2  morality-eMFD  \n",
       "3  morality-eMFD  \n",
       "4  morality-eMFD  \n",
       "5  morality-eMFD  \n",
       "0  morality-MFRC  \n",
       "1  morality-MFRC  \n",
       "2  morality-MFRC  \n",
       "3  morality-MFRC  \n",
       "4  morality-MFRC  \n",
       "5  morality-MFRC  \n",
       "0  morality-MFRC  \n",
       "1  morality-MFRC  \n",
       "2  morality-MFRC  \n",
       "3  morality-MFRC  \n",
       "4  morality-MFRC  \n",
       "5  morality-MFRC  \n",
       "0  morality-MFRC  \n",
       "1  morality-MFRC  \n",
       "2  morality-MFRC  \n",
       "3  morality-MFRC  \n",
       "4  morality-MFRC  \n",
       "5  morality-MFRC  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_ids_mftc = ['yon5adbv','2fisp7sj','pckcakff'] # MFTC\n",
    "run_ids_emfd = ['jv4exac1','d1kvjg6q','7krxewfl'] # eMFD\n",
    "run_ids_mfrc = ['jpsu9gfg','744jcvse','fq9kn2ok'] # MFRC\n",
    "\n",
    "run_ids = run_ids_mftc + run_ids_emfd + run_ids_mfrc\n",
    "\n",
    "df = []\n",
    "\n",
    "for run_id in tqdm(run_ids):\n",
    "\n",
    "    run = api.run(f\"{project_name}/{run_id}\")\n",
    "    artifact = api.artifact(f\"{project_name}/run-{run_id}-stats:latest\")\n",
    "    table = artifact.get(\"stats\").get_dataframe()\n",
    "    table['model_name'] = run.config['model_name']\n",
    "    table['data'] = run.config['test_data']\n",
    "    df.append( table )\n",
    "    \n",
    "df = pd.concat(df)\n",
    "df.rename({'index':'foundation'},axis=1,inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_row = df.groupby(['data','foundation'])[['tpr_mean','tnr_mean']].mean().reset_index()\n",
    "baseline_row.loc[:,'model_name'] = 'Human'\n",
    "baseline_row = baseline_row.rename({'tpr_mean':'tpr_ai','tnr_mean':'tnr_ai'},axis=1)\n",
    "baseline_row\n",
    "df = pd.concat([df, baseline_row], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df['foundation'] != 'any'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-1623dcd9ad3049cfb2618f73081f40c2.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-1623dcd9ad3049cfb2618f73081f40c2.vega-embed details,\n",
       "  #altair-viz-1623dcd9ad3049cfb2618f73081f40c2.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-1623dcd9ad3049cfb2618f73081f40c2\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-1623dcd9ad3049cfb2618f73081f40c2\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-1623dcd9ad3049cfb2618f73081f40c2\");\n",
       "    }\n",
       "\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      let deps = [\"vega-embed\"];\n",
       "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}, \"facet\": {\"columns\": 3}, \"legend\": {\"columns\": 2, \"orient\": \"top\", \"symbolLimit\": 0}}, \"data\": {\"name\": \"data-1fd79851d28b920a8ef694ef2a0f4704\"}, \"facet\": {\"column\": {\"field\": \"data\", \"title\": \"Dataset\", \"type\": \"nominal\"}}, \"spec\": {\"layer\": [{\"mark\": {\"type\": \"line\", \"color\": \"gray\", \"strokeDash\": [2, 2]}, \"encoding\": {\"x\": {\"field\": \"value\", \"scale\": {\"domain\": [0, 0.7]}, \"type\": \"quantitative\"}, \"y\": {\"field\": \"value\", \"scale\": {\"domain\": [0, 0.7]}, \"type\": \"quantitative\"}}, \"transform\": [{\"calculate\": \"sequence(0, 0.8, 0.1)\", \"as\": \"value\"}, {\"flatten\": [\"value\"]}]}, {\"mark\": {\"type\": \"point\", \"filled\": true, \"size\": 200, \"stroke\": \"white\", \"strokeWidth\": 2}, \"encoding\": {\"color\": {\"field\": \"foundation\", \"legend\": {\"title\": \"Moral Dimension\"}, \"scale\": {\"domain\": [\"authority\", \"care\", \"fairness\", \"loyalty\", \"sanctity\", \"any\"], \"range\": [\"#6A4C93\", \"#00B4A6\", \"#3498DB\", \"#E74C3C\", \"#F39C12\", \"gray\"]}, \"type\": \"nominal\"}, \"shape\": {\"field\": \"annotator\", \"legend\": {\"title\": \"Model\"}, \"scale\": {\"domain\": [\"Human Baseline\", \"Claude-4\", \"DeepSeek-V3\", \"Llama4-Maverick\"], \"range\": [\"circle\", \"square\", \"triangle-up\", \"diamond\"]}, \"type\": \"nominal\"}, \"tooltip\": [{\"field\": \"foundation\", \"type\": \"nominal\"}, {\"field\": \"annotator\", \"type\": \"nominal\"}, {\"field\": \"fpr\", \"type\": \"quantitative\"}, {\"field\": \"fnr\", \"type\": \"quantitative\"}], \"x\": {\"axis\": {\"format\": \".0%\"}, \"field\": \"jitter_x\", \"scale\": {\"domain\": [0.7, 0.7]}, \"title\": \"False Positive Rate\", \"type\": \"quantitative\"}, \"y\": {\"axis\": {\"format\": \".0%\"}, \"field\": \"jitter_y\", \"scale\": {\"domain\": [0.7, 0.7]}, \"title\": \"False Negative Rate\", \"type\": \"quantitative\"}}, \"transform\": [{\"calculate\": \"datum.fpr + (random() - 0.5) * 0.01\", \"as\": \"jitter_x\"}, {\"calculate\": \"datum.fnr + (random() - 0.5) * 0.01\", \"as\": \"jitter_y\"}]}], \"height\": 200, \"width\": 300}, \"resolve\": {\"scale\": {\"color\": \"shared\", \"shape\": \"shared\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-1fd79851d28b920a8ef694ef2a0f4704\": [{\"foundation\": \"care\", \"tpr_ai_rank\": 83.33333333333334, \"tnr_ai_rank\": 4.166666666666667, \"tpr_mean\": 0.5038381218910217, \"tnr_mean\": 0.9325043559074402, \"ba_mean\": 0.7181711792945862, \"ba_ai_rank\": 70.83333333333334, \"tpr_ai\": 0.848118007183075, \"tnr_ai\": 0.7152314782142639, \"ba_ai\": 0.7816747426986694, \"pi_1\": 0.2831953465938568, \"model_name\": \"deepseek-v3\", \"data\": \"MFTC\", \"fpr\": 0.2847685217857361, \"fnr\": 0.15188199281692505, \"annotator\": \"DeepSeek-V3\"}, {\"foundation\": \"fairness\", \"tpr_ai_rank\": 66.66666666666667, \"tnr_ai_rank\": 29.166666666666668, \"tpr_mean\": 0.5716874599456787, \"tnr_mean\": 0.9369329810142517, \"ba_mean\": 0.7543101906776428, \"ba_ai_rank\": 70.83333333333334, \"tpr_ai\": 0.7137046456336975, \"tnr_ai\": 0.9130122065544128, \"ba_ai\": 0.8133584260940552, \"pi_1\": 0.22159399092197418, \"model_name\": \"deepseek-v3\", \"data\": \"MFTC\", \"fpr\": 0.08698779344558716, \"fnr\": 0.2862953543663025, \"annotator\": \"DeepSeek-V3\"}, {\"foundation\": \"loyalty\", \"tpr_ai_rank\": 83.33333333333334, \"tnr_ai_rank\": 12.5, \"tpr_mean\": 0.5348414778709412, \"tnr_mean\": 0.9148320555686951, \"ba_mean\": 0.7248366475105286, \"ba_ai_rank\": 79.16666666666667, \"tpr_ai\": 0.7710886001586914, \"tnr_ai\": 0.8327292203903198, \"ba_ai\": 0.8019089102745056, \"pi_1\": 0.11592563986778259, \"model_name\": \"deepseek-v3\", \"data\": \"MFTC\", \"fpr\": 0.16727077960968018, \"fnr\": 0.2289113998413086, \"annotator\": \"DeepSeek-V3\"}, {\"foundation\": \"authority\", \"tpr_ai_rank\": 75.0, \"tnr_ai_rank\": 29.166666666666668, \"tpr_mean\": 0.4687623977661133, \"tnr_mean\": 0.8670685887336731, \"ba_mean\": 0.667915403842926, \"ba_ai_rank\": 87.5, \"tpr_ai\": 0.7537100911140442, \"tnr_ai\": 0.88910973072052, \"ba_ai\": 0.8214099407196045, \"pi_1\": 0.18744805455207825, \"model_name\": \"deepseek-v3\", \"data\": \"MFTC\", \"fpr\": 0.11089026927947998, \"fnr\": 0.2462899088859558, \"annotator\": \"DeepSeek-V3\"}, {\"foundation\": \"sanctity\", \"tpr_ai_rank\": 83.33333333333334, \"tnr_ai_rank\": 41.66666666666667, \"tpr_mean\": 0.42097795009613037, \"tnr_mean\": 0.9269197583198547, \"ba_mean\": 0.6739489436149597, \"ba_ai_rank\": 91.66666666666667, \"tpr_ai\": 0.7327820062637329, \"tnr_ai\": 0.9624252319335938, \"ba_ai\": 0.8476036190986633, \"pi_1\": 0.14835606515407562, \"model_name\": \"deepseek-v3\", \"data\": \"MFTC\", \"fpr\": 0.03757476806640625, \"fnr\": 0.2672179937362671, \"annotator\": \"DeepSeek-V3\"}, {\"foundation\": \"care\", \"tpr_ai_rank\": 83.33333333333334, \"tnr_ai_rank\": 4.166666666666667, \"tpr_mean\": 0.5075859427452087, \"tnr_mean\": 0.9318777918815613, \"ba_mean\": 0.719731867313385, \"ba_ai_rank\": 62.50000000000001, \"tpr_ai\": 0.8009536862373352, \"tnr_ai\": 0.7183328866958618, \"ba_ai\": 0.7596433162689209, \"pi_1\": 0.2772005796432495, \"model_name\": \"llama4_maverick\", \"data\": \"MFTC\", \"fpr\": 0.2816671133041382, \"fnr\": 0.1990463137626648, \"annotator\": \"Llama4-Maverick\"}, {\"foundation\": \"fairness\", \"tpr_ai_rank\": 79.16666666666667, \"tnr_ai_rank\": 12.5, \"tpr_mean\": 0.5577060580253601, \"tnr_mean\": 0.939609706401825, \"ba_mean\": 0.7486578822135925, \"ba_ai_rank\": 70.83333333333334, \"tpr_ai\": 0.7719876766204834, \"tnr_ai\": 0.8850435018539429, \"ba_ai\": 0.8285155892372131, \"pi_1\": 0.22885295748710632, \"model_name\": \"llama4_maverick\", \"data\": \"MFTC\", \"fpr\": 0.11495649814605713, \"fnr\": 0.2280123233795166, \"annotator\": \"Llama4-Maverick\"}, {\"foundation\": \"loyalty\", \"tpr_ai_rank\": 95.83333333333334, \"tnr_ai_rank\": 8.333333333333334, \"tpr_mean\": 0.5308359265327454, \"tnr_mean\": 0.9103533625602722, \"ba_mean\": 0.7205946445465088, \"ba_ai_rank\": 75.0, \"tpr_ai\": 0.8671379089355469, \"tnr_ai\": 0.7229380011558533, \"ba_ai\": 0.7950379848480225, \"pi_1\": 0.11896416544914246, \"model_name\": \"llama4_maverick\", \"data\": \"MFTC\", \"fpr\": 0.27706199884414673, \"fnr\": 0.13286209106445312, \"annotator\": \"Llama4-Maverick\"}, {\"foundation\": \"authority\", \"tpr_ai_rank\": 75.0, \"tnr_ai_rank\": 29.166666666666668, \"tpr_mean\": 0.46326589584350586, \"tnr_mean\": 0.8675575852394104, \"ba_mean\": 0.6654117703437805, \"ba_ai_rank\": 79.16666666666667, \"tpr_ai\": 0.7610557079315186, \"tnr_ai\": 0.8204398155212402, \"ba_ai\": 0.7907477617263794, \"pi_1\": 0.18976156413555145, \"model_name\": \"llama4_maverick\", \"data\": \"MFTC\", \"fpr\": 0.17956018447875977, \"fnr\": 0.23894429206848145, \"annotator\": \"Llama4-Maverick\"}, {\"foundation\": \"sanctity\", \"tpr_ai_rank\": 91.66666666666667, \"tnr_ai_rank\": 20.833333333333336, \"tpr_mean\": 0.4013499319553375, \"tnr_mean\": 0.9274177551269531, \"ba_mean\": 0.6643838882446289, \"ba_ai_rank\": 95.83333333333334, \"tpr_ai\": 0.8389440774917603, \"tnr_ai\": 0.9077293276786804, \"ba_ai\": 0.873336672782898, \"pi_1\": 0.18530641496181488, \"model_name\": \"llama4_maverick\", \"data\": \"MFTC\", \"fpr\": 0.09227067232131958, \"fnr\": 0.16105592250823975, \"annotator\": \"Llama4-Maverick\"}, {\"foundation\": \"care\", \"tpr_ai_rank\": 95.83333333333334, \"tnr_ai_rank\": 4.166666666666667, \"tpr_mean\": 0.47980013489723206, \"tnr_mean\": 0.9291448593139648, \"ba_mean\": 0.7044724822044373, \"ba_ai_rank\": 75.0, \"tpr_ai\": 0.9278887510299683, \"tnr_ai\": 0.6419569849967957, \"ba_ai\": 0.7849228382110596, \"pi_1\": 0.30288422107696533, \"model_name\": \"claude-4-sonnet\", \"data\": \"MFTC\", \"fpr\": 0.35804301500320435, \"fnr\": 0.07211124897003174, \"annotator\": \"Claude-4\"}, {\"foundation\": \"fairness\", \"tpr_ai_rank\": 95.83333333333334, \"tnr_ai_rank\": 4.166666666666667, \"tpr_mean\": 0.5641373991966248, \"tnr_mean\": 0.9364112019538879, \"ba_mean\": 0.7502743601799011, \"ba_ai_rank\": 70.83333333333334, \"tpr_ai\": 0.9067123532295227, \"tnr_ai\": 0.7418163418769836, \"ba_ai\": 0.8242643475532532, \"pi_1\": 0.23713313043117523, \"model_name\": \"claude-4-sonnet\", \"data\": \"MFTC\", \"fpr\": 0.25818365812301636, \"fnr\": 0.0932876467704773, \"annotator\": \"Claude-4\"}, {\"foundation\": \"loyalty\", \"tpr_ai_rank\": 83.33333333333334, \"tnr_ai_rank\": 12.5, \"tpr_mean\": 0.5267181992530823, \"tnr_mean\": 0.9159168601036072, \"ba_mean\": 0.7213175892829895, \"ba_ai_rank\": 79.16666666666667, \"tpr_ai\": 0.7985854148864746, \"tnr_ai\": 0.8129532933235168, \"ba_ai\": 0.8057693243026733, \"pi_1\": 0.12112497538328171, \"model_name\": \"claude-4-sonnet\", \"data\": \"MFTC\", \"fpr\": 0.18704670667648315, \"fnr\": 0.2014145851135254, \"annotator\": \"Claude-4\"}, {\"foundation\": \"authority\", \"tpr_ai_rank\": 87.5, \"tnr_ai_rank\": 25.0, \"tpr_mean\": 0.47630205750465393, \"tnr_mean\": 0.8647470474243164, \"ba_mean\": 0.6705245971679688, \"ba_ai_rank\": 83.33333333333334, \"tpr_ai\": 0.8528195023536682, \"tnr_ai\": 0.7471603751182556, \"ba_ai\": 0.7999899387359619, \"pi_1\": 0.19189341366291046, \"model_name\": \"claude-4-sonnet\", \"data\": \"MFTC\", \"fpr\": 0.2528396248817444, \"fnr\": 0.1471804976463318, \"annotator\": \"Claude-4\"}, {\"foundation\": \"sanctity\", \"tpr_ai_rank\": 100.0, \"tnr_ai_rank\": 20.833333333333336, \"tpr_mean\": 0.405719131231308, \"tnr_mean\": 0.9258866310119629, \"ba_mean\": 0.6658028960227966, \"ba_ai_rank\": 100.0, \"tpr_ai\": 0.9219385981559753, \"tnr_ai\": 0.8781507611274719, \"ba_ai\": 0.9000446796417236, \"pi_1\": 0.1850157529115677, \"model_name\": \"claude-4-sonnet\", \"data\": \"MFTC\", \"fpr\": 0.12184923887252808, \"fnr\": 0.07806140184402466, \"annotator\": \"Claude-4\"}, {\"foundation\": \"care\", \"tpr_ai_rank\": 97.09923664122138, \"tnr_ai_rank\": 46.25954198473283, \"tpr_mean\": 0.39866262674331665, \"tnr_mean\": 0.8473599553108215, \"ba_mean\": 0.6230112910270691, \"ba_ai_rank\": 97.55725190839695, \"tpr_ai\": 0.9207785129547119, \"tnr_ai\": 0.859453558921814, \"ba_ai\": 0.8901160359382629, \"pi_1\": 0.26113852858543396, \"model_name\": \"llama4_maverick\", \"data\": \"eMFD\", \"fpr\": 0.14054644107818604, \"fnr\": 0.07922148704528809, \"annotator\": \"Llama4-Maverick\"}, {\"foundation\": \"fairness\", \"tpr_ai_rank\": 87.17557251908397, \"tnr_ai_rank\": 52.06106870229008, \"tpr_mean\": 0.4584009647369385, \"tnr_mean\": 0.8389748334884644, \"ba_mean\": 0.6486878991127014, \"ba_ai_rank\": 88.09160305343512, \"tpr_ai\": 0.8184910416603088, \"tnr_ai\": 0.8660367727279663, \"ba_ai\": 0.84226393699646, \"pi_1\": 0.18375174701213837, \"model_name\": \"llama4_maverick\", \"data\": \"eMFD\", \"fpr\": 0.1339632272720337, \"fnr\": 0.18150895833969116, \"annotator\": \"Llama4-Maverick\"}, {\"foundation\": \"loyalty\", \"tpr_ai_rank\": 90.83969465648855, \"tnr_ai_rank\": 46.56488549618321, \"tpr_mean\": 0.4147929549217224, \"tnr_mean\": 0.837053656578064, \"ba_mean\": 0.6259233355522156, \"ba_ai_rank\": 91.45038167938931, \"tpr_ai\": 0.8006342053413391, \"tnr_ai\": 0.8520230054855347, \"ba_ai\": 0.8263286352157593, \"pi_1\": 0.19533739984035492, \"model_name\": \"llama4_maverick\", \"data\": \"eMFD\", \"fpr\": 0.14797699451446533, \"fnr\": 0.1993657946586609, \"annotator\": \"Llama4-Maverick\"}, {\"foundation\": \"authority\", \"tpr_ai_rank\": 94.80916030534351, \"tnr_ai_rank\": 28.244274809160306, \"tpr_mean\": 0.43119707703590393, \"tnr_mean\": 0.8429122567176819, \"ba_mean\": 0.6370546817779541, \"ba_ai_rank\": 92.36641221374046, \"tpr_ai\": 0.8743069767951965, \"tnr_ai\": 0.7929282188415527, \"ba_ai\": 0.8336175680160522, \"pi_1\": 0.23753905296325684, \"model_name\": \"llama4_maverick\", \"data\": \"eMFD\", \"fpr\": 0.20707178115844727, \"fnr\": 0.12569302320480347, \"annotator\": \"Llama4-Maverick\"}, {\"foundation\": \"sanctity\", \"tpr_ai_rank\": 80.76335877862596, \"tnr_ai_rank\": 79.8473282442748, \"tpr_mean\": 0.3827056884765625, \"tnr_mean\": 0.8635557889938354, \"ba_mean\": 0.623130738735199, \"ba_ai_rank\": 85.95419847328245, \"tpr_ai\": 0.6775403022766113, \"tnr_ai\": 0.9620656371116638, \"ba_ai\": 0.81980299949646, \"pi_1\": 0.10252536088228226, \"model_name\": \"llama4_maverick\", \"data\": \"eMFD\", \"fpr\": 0.03793436288833618, \"fnr\": 0.32245969772338867, \"annotator\": \"Llama4-Maverick\"}, {\"foundation\": \"care\", \"tpr_ai_rank\": 96.03053435114504, \"tnr_ai_rank\": 40.61068702290076, \"tpr_mean\": 0.40262508392333984, \"tnr_mean\": 0.8476040959358215, \"ba_mean\": 0.6251145601272583, \"ba_ai_rank\": 96.79389312977099, \"tpr_ai\": 0.9088319540023804, \"tnr_ai\": 0.8477559685707092, \"ba_ai\": 0.8782939910888672, \"pi_1\": 0.26049160957336426, \"model_name\": \"claude-4-sonnet\", \"data\": \"eMFD\", \"fpr\": 0.15224403142929077, \"fnr\": 0.09116804599761963, \"annotator\": \"Claude-4\"}, {\"foundation\": \"fairness\", \"tpr_ai_rank\": 90.0763358778626, \"tnr_ai_rank\": 47.02290076335878, \"tpr_mean\": 0.42983776330947876, \"tnr_mean\": 0.8369385004043579, \"ba_mean\": 0.633388102054596, \"ba_ai_rank\": 90.99236641221374, \"tpr_ai\": 0.8072920441627502, \"tnr_ai\": 0.8496489524841309, \"ba_ai\": 0.8284704685211182, \"pi_1\": 0.18413299322128296, \"model_name\": \"claude-4-sonnet\", \"data\": \"eMFD\", \"fpr\": 0.15035104751586914, \"fnr\": 0.19270795583724976, \"annotator\": \"Claude-4\"}, {\"foundation\": \"loyalty\", \"tpr_ai_rank\": 75.1145038167939, \"tnr_ai_rank\": 57.099236641221374, \"tpr_mean\": 0.3919679522514343, \"tnr_mean\": 0.8370065093040466, \"ba_mean\": 0.6144872307777405, \"ba_ai_rank\": 78.4732824427481, \"tpr_ai\": 0.551091730594635, \"tnr_ai\": 0.8941047787666321, \"ba_ai\": 0.7225982546806335, \"pi_1\": 0.21498408913612366, \"model_name\": \"claude-4-sonnet\", \"data\": \"eMFD\", \"fpr\": 0.10589522123336792, \"fnr\": 0.448908269405365, \"annotator\": \"Claude-4\"}, {\"foundation\": \"authority\", \"tpr_ai_rank\": 76.48854961832062, \"tnr_ai_rank\": 37.55725190839695, \"tpr_mean\": 0.4353860020637512, \"tnr_mean\": 0.8367228507995605, \"ba_mean\": 0.6360543966293335, \"ba_ai_rank\": 74.9618320610687, \"tpr_ai\": 0.6635054349899292, \"tnr_ai\": 0.8159849047660828, \"ba_ai\": 0.7397451400756836, \"pi_1\": 0.21249820291996002, \"model_name\": \"claude-4-sonnet\", \"data\": \"eMFD\", \"fpr\": 0.18401509523391724, \"fnr\": 0.3364945650100708, \"annotator\": \"Claude-4\"}, {\"foundation\": \"sanctity\", \"tpr_ai_rank\": 77.55725190839695, \"tnr_ai_rank\": 76.03053435114504, \"tpr_mean\": 0.39615869522094727, \"tnr_mean\": 0.8665280342102051, \"ba_mean\": 0.631343424320221, \"ba_ai_rank\": 83.5114503816794, \"tpr_ai\": 0.6385132074356079, \"tnr_ai\": 0.9589788317680359, \"ba_ai\": 0.7987459897994995, \"pi_1\": 0.10448930412530899, \"model_name\": \"claude-4-sonnet\", \"data\": \"eMFD\", \"fpr\": 0.04102116823196411, \"fnr\": 0.3614867925643921, \"annotator\": \"Claude-4\"}, {\"foundation\": \"care\", \"tpr_ai_rank\": 97.09923664122138, \"tnr_ai_rank\": 38.3206106870229, \"tpr_mean\": 0.40424758195877075, \"tnr_mean\": 0.847732663154602, \"ba_mean\": 0.625990092754364, \"ba_ai_rank\": 97.55725190839695, \"tpr_ai\": 0.9248145222663879, \"tnr_ai\": 0.8396958112716675, \"ba_ai\": 0.8822551965713501, \"pi_1\": 0.26718801259994507, \"model_name\": \"deepseek-v3\", \"data\": \"eMFD\", \"fpr\": 0.16030418872833252, \"fnr\": 0.07518547773361206, \"annotator\": \"DeepSeek-V3\"}, {\"foundation\": \"fairness\", \"tpr_ai_rank\": 79.54198473282443, \"tnr_ai_rank\": 58.93129770992367, \"tpr_mean\": 0.46509525179862976, \"tnr_mean\": 0.837304949760437, \"ba_mean\": 0.6512001156806946, \"ba_ai_rank\": 85.19083969465649, \"tpr_ai\": 0.7504355907440186, \"tnr_ai\": 0.8845329284667969, \"ba_ai\": 0.8174842596054077, \"pi_1\": 0.17233768105506897, \"model_name\": \"deepseek-v3\", \"data\": \"eMFD\", \"fpr\": 0.11546707153320312, \"fnr\": 0.24956440925598145, \"annotator\": \"DeepSeek-V3\"}, {\"foundation\": \"loyalty\", \"tpr_ai_rank\": 92.51908396946565, \"tnr_ai_rank\": 54.656488549618324, \"tpr_mean\": 0.4108866751194, \"tnr_mean\": 0.8341456651687622, \"ba_mean\": 0.6225162148475647, \"ba_ai_rank\": 93.28244274809161, \"tpr_ai\": 0.8318749070167542, \"tnr_ai\": 0.87030428647995, \"ba_ai\": 0.851089596748352, \"pi_1\": 0.18746206164360046, \"model_name\": \"deepseek-v3\", \"data\": \"eMFD\", \"fpr\": 0.12969571352005005, \"fnr\": 0.16812509298324585, \"annotator\": \"DeepSeek-V3\"}, {\"foundation\": \"authority\", \"tpr_ai_rank\": 90.83969465648855, \"tnr_ai_rank\": 32.51908396946565, \"tpr_mean\": 0.43832820653915405, \"tnr_mean\": 0.841079831123352, \"ba_mean\": 0.6397040486335754, \"ba_ai_rank\": 89.16030534351145, \"tpr_ai\": 0.8177727460861206, \"tnr_ai\": 0.8100284934043884, \"ba_ai\": 0.8139005899429321, \"pi_1\": 0.22688040137290955, \"model_name\": \"deepseek-v3\", \"data\": \"eMFD\", \"fpr\": 0.18997150659561157, \"fnr\": 0.1822272539138794, \"annotator\": \"DeepSeek-V3\"}, {\"foundation\": \"sanctity\", \"tpr_ai_rank\": 69.46564885496183, \"tnr_ai_rank\": 84.27480916030535, \"tpr_mean\": 0.38546785712242126, \"tnr_mean\": 0.8639007210731506, \"ba_mean\": 0.6246842741966248, \"ba_ai_rank\": 76.94656488549619, \"tpr_ai\": 0.5223512053489685, \"tnr_ai\": 0.9758826494216919, \"ba_ai\": 0.7491168975830078, \"pi_1\": 0.10076507180929184, \"model_name\": \"deepseek-v3\", \"data\": \"eMFD\", \"fpr\": 0.024117350578308105, \"fnr\": 0.4776487946510315, \"annotator\": \"DeepSeek-V3\"}, {\"foundation\": \"care\", \"tpr_ai_rank\": 100.0, \"tnr_ai_rank\": 14.285714285714286, \"tpr_mean\": 0.5827096700668335, \"tnr_mean\": 0.9455891251564026, \"ba_mean\": 0.7641493678092957, \"ba_ai_rank\": 100.0, \"tpr_ai\": 0.9271491169929504, \"tnr_ai\": 0.8629359602928162, \"ba_ai\": 0.8950425386428833, \"pi_1\": 0.1810387670993805, \"model_name\": \"deepseek-v3\", \"data\": \"MFRC\", \"fpr\": 0.13706403970718384, \"fnr\": 0.07285088300704956, \"annotator\": \"DeepSeek-V3\"}, {\"foundation\": \"fairness\", \"tpr_ai_rank\": 71.42857142857143, \"tnr_ai_rank\": 28.571428571428573, \"tpr_mean\": 0.5869225859642029, \"tnr_mean\": 0.9137951731681824, \"ba_mean\": 0.7503588795661926, \"ba_ai_rank\": 42.85714285714286, \"tpr_ai\": 0.6358491778373718, \"tnr_ai\": 0.8203585743904114, \"ba_ai\": 0.7281038761138916, \"pi_1\": 0.1540202796459198, \"model_name\": \"deepseek-v3\", \"data\": \"MFRC\", \"fpr\": 0.17964142560958862, \"fnr\": 0.3641508221626282, \"annotator\": \"DeepSeek-V3\"}, {\"foundation\": \"loyalty\", \"tpr_ai_rank\": 100.0, \"tnr_ai_rank\": 14.285714285714286, \"tpr_mean\": 0.4829337000846863, \"tnr_mean\": 0.9455137848854065, \"ba_mean\": 0.7142237424850464, \"ba_ai_rank\": 85.71428571428572, \"tpr_ai\": 0.9037629961967468, \"tnr_ai\": 0.7856495976448059, \"ba_ai\": 0.8447062969207764, \"pi_1\": 0.09072095155715942, \"model_name\": \"deepseek-v3\", \"data\": \"MFRC\", \"fpr\": 0.2143504023551941, \"fnr\": 0.09623700380325317, \"annotator\": \"DeepSeek-V3\"}, {\"foundation\": \"authority\", \"tpr_ai_rank\": 100.0, \"tnr_ai_rank\": 14.285714285714286, \"tpr_mean\": 0.4418303966522217, \"tnr_mean\": 0.9523071050643921, \"ba_mean\": 0.6970688104629517, \"ba_ai_rank\": 85.71428571428572, \"tpr_ai\": 0.8128089904785156, \"tnr_ai\": 0.8563677668571472, \"ba_ai\": 0.8345884084701538, \"pi_1\": 0.16532497107982635, \"model_name\": \"deepseek-v3\", \"data\": \"MFRC\", \"fpr\": 0.14363223314285278, \"fnr\": 0.18719100952148438, \"annotator\": \"DeepSeek-V3\"}, {\"foundation\": \"sanctity\", \"tpr_ai_rank\": 100.0, \"tnr_ai_rank\": 42.85714285714286, \"tpr_mean\": 0.45229077339172363, \"tnr_mean\": 0.9674052000045776, \"ba_mean\": 0.7098479866981506, \"ba_ai_rank\": 100.0, \"tpr_ai\": 0.6872663497924805, \"tnr_ai\": 0.9553496241569519, \"ba_ai\": 0.8213080167770386, \"pi_1\": 0.04295072332024574, \"model_name\": \"deepseek-v3\", \"data\": \"MFRC\", \"fpr\": 0.044650375843048096, \"fnr\": 0.31273365020751953, \"annotator\": \"DeepSeek-V3\"}, {\"foundation\": \"care\", \"tpr_ai_rank\": 100.0, \"tnr_ai_rank\": 14.285714285714286, \"tpr_mean\": 0.5628657341003418, \"tnr_mean\": 0.9475950598716736, \"ba_mean\": 0.7552304267883301, \"ba_ai_rank\": 100.0, \"tpr_ai\": 0.9471412897109985, \"tnr_ai\": 0.8477157950401306, \"ba_ai\": 0.8974285125732422, \"pi_1\": 0.2012070268392563, \"model_name\": \"claude-4-sonnet\", \"data\": \"MFRC\", \"fpr\": 0.15228420495986938, \"fnr\": 0.052858710289001465, \"annotator\": \"Claude-4\"}, {\"foundation\": \"fairness\", \"tpr_ai_rank\": 100.0, \"tnr_ai_rank\": 14.285714285714286, \"tpr_mean\": 0.5979560017585754, \"tnr_mean\": 0.9054022431373596, \"ba_mean\": 0.7516790628433228, \"ba_ai_rank\": 85.71428571428572, \"tpr_ai\": 0.877366304397583, \"tnr_ai\": 0.7283533811569214, \"ba_ai\": 0.8028598427772522, \"pi_1\": 0.17254643142223358, \"model_name\": \"claude-4-sonnet\", \"data\": \"MFRC\", \"fpr\": 0.2716466188430786, \"fnr\": 0.12263369560241699, \"annotator\": \"Claude-4\"}, {\"foundation\": \"loyalty\", \"tpr_ai_rank\": 100.0, \"tnr_ai_rank\": 14.285714285714286, \"tpr_mean\": 0.4753001630306244, \"tnr_mean\": 0.950501024723053, \"ba_mean\": 0.7129005789756775, \"ba_ai_rank\": 85.71428571428572, \"tpr_ai\": 0.912767767906189, \"tnr_ai\": 0.8048187494277954, \"ba_ai\": 0.8587932586669922, \"pi_1\": 0.09632404893636703, \"model_name\": \"claude-4-sonnet\", \"data\": \"MFRC\", \"fpr\": 0.1951812505722046, \"fnr\": 0.08723223209381104, \"annotator\": \"Claude-4\"}, {\"foundation\": \"authority\", \"tpr_ai_rank\": 100.0, \"tnr_ai_rank\": 14.285714285714286, \"tpr_mean\": 0.42206913232803345, \"tnr_mean\": 0.9475111365318298, \"ba_mean\": 0.6847901344299316, \"ba_ai_rank\": 100.0, \"tpr_ai\": 0.8347545862197876, \"tnr_ai\": 0.8277637958526611, \"ba_ai\": 0.8312591910362244, \"pi_1\": 0.16522625088691711, \"model_name\": \"claude-4-sonnet\", \"data\": \"MFRC\", \"fpr\": 0.17223620414733887, \"fnr\": 0.1652454137802124, \"annotator\": \"Claude-4\"}, {\"foundation\": \"sanctity\", \"tpr_ai_rank\": 100.0, \"tnr_ai_rank\": 14.285714285714286, \"tpr_mean\": 0.44499775767326355, \"tnr_mean\": 0.9617622494697571, \"ba_mean\": 0.7033800482749939, \"ba_ai_rank\": 100.0, \"tpr_ai\": 0.9198097586631775, \"tnr_ai\": 0.9053412675857544, \"ba_ai\": 0.9125754833221436, \"pi_1\": 0.056389521807432175, \"model_name\": \"claude-4-sonnet\", \"data\": \"MFRC\", \"fpr\": 0.0946587324142456, \"fnr\": 0.08019024133682251, \"annotator\": \"Claude-4\"}, {\"foundation\": \"care\", \"tpr_ai_rank\": 100.0, \"tnr_ai_rank\": 28.571428571428573, \"tpr_mean\": 0.5823439359664917, \"tnr_mean\": 0.9476696252822876, \"ba_mean\": 0.7650067210197449, \"ba_ai_rank\": 100.0, \"tpr_ai\": 0.8923664689064026, \"tnr_ai\": 0.8857666850090027, \"ba_ai\": 0.8890665769577026, \"pi_1\": 0.177754208445549, \"model_name\": \"llama4_maverick\", \"data\": \"MFRC\", \"fpr\": 0.11423331499099731, \"fnr\": 0.10763353109359741, \"annotator\": \"Llama4-Maverick\"}, {\"foundation\": \"fairness\", \"tpr_ai_rank\": 71.42857142857143, \"tnr_ai_rank\": 14.285714285714286, \"tpr_mean\": 0.5892537832260132, \"tnr_mean\": 0.9002681970596313, \"ba_mean\": 0.7447609901428223, \"ba_ai_rank\": 42.85714285714286, \"tpr_ai\": 0.7188828587532043, \"tnr_ai\": 0.7123743295669556, \"ba_ai\": 0.7156286239624023, \"pi_1\": 0.16355669498443604, \"model_name\": \"llama4_maverick\", \"data\": \"MFRC\", \"fpr\": 0.28762567043304443, \"fnr\": 0.28111714124679565, \"annotator\": \"Llama4-Maverick\"}, {\"foundation\": \"loyalty\", \"tpr_ai_rank\": 100.0, \"tnr_ai_rank\": 14.285714285714286, \"tpr_mean\": 0.4694995880126953, \"tnr_mean\": 0.9423810839653015, \"ba_mean\": 0.7059403657913208, \"ba_ai_rank\": 85.71428571428572, \"tpr_ai\": 0.9026899933815002, \"tnr_ai\": 0.75157630443573, \"ba_ai\": 0.8271331787109375, \"pi_1\": 0.09922366589307785, \"model_name\": \"llama4_maverick\", \"data\": \"MFRC\", \"fpr\": 0.24842369556427002, \"fnr\": 0.09731000661849976, \"annotator\": \"Llama4-Maverick\"}, {\"foundation\": \"authority\", \"tpr_ai_rank\": 100.0, \"tnr_ai_rank\": 14.285714285714286, \"tpr_mean\": 0.44115617871284485, \"tnr_mean\": 0.9420163035392761, \"ba_mean\": 0.6915861964225769, \"ba_ai_rank\": 100.0, \"tpr_ai\": 0.8540239930152893, \"tnr_ai\": 0.8000373840332031, \"ba_ai\": 0.8270306587219238, \"pi_1\": 0.15614083409309387, \"model_name\": \"llama4_maverick\", \"data\": \"MFRC\", \"fpr\": 0.19996261596679688, \"fnr\": 0.1459760069847107, \"annotator\": \"Llama4-Maverick\"}, {\"foundation\": \"sanctity\", \"tpr_ai_rank\": 100.0, \"tnr_ai_rank\": 14.285714285714286, \"tpr_mean\": 0.44053807854652405, \"tnr_mean\": 0.9598873853683472, \"ba_mean\": 0.7002127766609192, \"ba_ai_rank\": 100.0, \"tpr_ai\": 0.8479808568954468, \"tnr_ai\": 0.8950592279434204, \"ba_ai\": 0.8715200424194336, \"pi_1\": 0.05148153007030487, \"model_name\": \"llama4_maverick\", \"data\": \"MFRC\", \"fpr\": 0.10494077205657959, \"fnr\": 0.15201914310455322, \"annotator\": \"Llama4-Maverick\"}, {\"foundation\": \"authority\", \"tpr_ai_rank\": null, \"tnr_ai_rank\": null, \"tpr_mean\": null, \"tnr_mean\": null, \"ba_mean\": null, \"ba_ai_rank\": null, \"tpr_ai\": 0.4350185692310333, \"tnr_ai\": 0.9472781817118326, \"ba_ai\": null, \"pi_1\": null, \"model_name\": \"Human\", \"data\": \"MFRC\", \"fpr\": 0.052721818288167355, \"fnr\": 0.5649814307689667, \"annotator\": \"Human Baseline\"}, {\"foundation\": \"care\", \"tpr_ai_rank\": null, \"tnr_ai_rank\": null, \"tpr_mean\": null, \"tnr_mean\": null, \"ba_mean\": null, \"ba_ai_rank\": null, \"tpr_ai\": 0.575973113377889, \"tnr_ai\": 0.9469512701034546, \"ba_ai\": null, \"pi_1\": null, \"model_name\": \"Human\", \"data\": \"MFRC\", \"fpr\": 0.05304872989654541, \"fnr\": 0.42402688662211097, \"annotator\": \"Human Baseline\"}, {\"foundation\": \"fairness\", \"tpr_ai_rank\": null, \"tnr_ai_rank\": null, \"tpr_mean\": null, \"tnr_mean\": null, \"ba_mean\": null, \"ba_ai_rank\": null, \"tpr_ai\": 0.5913774569829305, \"tnr_ai\": 0.9064885377883911, \"ba_ai\": null, \"pi_1\": null, \"model_name\": \"Human\", \"data\": \"MFRC\", \"fpr\": 0.09351146221160889, \"fnr\": 0.40862254301706946, \"annotator\": \"Human Baseline\"}, {\"foundation\": \"loyalty\", \"tpr_ai_rank\": null, \"tnr_ai_rank\": null, \"tpr_mean\": null, \"tnr_mean\": null, \"ba_mean\": null, \"ba_ai_rank\": null, \"tpr_ai\": 0.475911150376002, \"tnr_ai\": 0.946131964524587, \"ba_ai\": null, \"pi_1\": null, \"model_name\": \"Human\", \"data\": \"MFRC\", \"fpr\": 0.05386803547541297, \"fnr\": 0.5240888496239979, \"annotator\": \"Human Baseline\"}, {\"foundation\": \"sanctity\", \"tpr_ai_rank\": null, \"tnr_ai_rank\": null, \"tpr_mean\": null, \"tnr_mean\": null, \"ba_mean\": null, \"ba_ai_rank\": null, \"tpr_ai\": 0.4459422032038371, \"tnr_ai\": 0.9630182782808939, \"ba_ai\": null, \"pi_1\": null, \"model_name\": \"Human\", \"data\": \"MFRC\", \"fpr\": 0.036981721719106075, \"fnr\": 0.5540577967961628, \"annotator\": \"Human Baseline\"}, {\"foundation\": \"authority\", \"tpr_ai_rank\": null, \"tnr_ai_rank\": null, \"tpr_mean\": null, \"tnr_mean\": null, \"ba_mean\": null, \"ba_ai_rank\": null, \"tpr_ai\": 0.4694434503714244, \"tnr_ai\": 0.8664577404657999, \"ba_ai\": null, \"pi_1\": null, \"model_name\": \"Human\", \"data\": \"MFTC\", \"fpr\": 0.13354225953420007, \"fnr\": 0.5305565496285756, \"annotator\": \"Human Baseline\"}, {\"foundation\": \"care\", \"tpr_ai_rank\": null, \"tnr_ai_rank\": null, \"tpr_mean\": null, \"tnr_mean\": null, \"ba_mean\": null, \"ba_ai_rank\": null, \"tpr_ai\": 0.49707473317782086, \"tnr_ai\": 0.9311756690343221, \"ba_ai\": null, \"pi_1\": null, \"model_name\": \"Human\", \"data\": \"MFTC\", \"fpr\": 0.06882433096567786, \"fnr\": 0.5029252668221791, \"annotator\": \"Human Baseline\"}, {\"foundation\": \"fairness\", \"tpr_ai_rank\": null, \"tnr_ai_rank\": null, \"tpr_mean\": null, \"tnr_mean\": null, \"ba_mean\": null, \"ba_ai_rank\": null, \"tpr_ai\": 0.5645103057225546, \"tnr_ai\": 0.9376512964566549, \"ba_ai\": null, \"pi_1\": null, \"model_name\": \"Human\", \"data\": \"MFTC\", \"fpr\": 0.062348703543345096, \"fnr\": 0.43548969427744544, \"annotator\": \"Human Baseline\"}, {\"foundation\": \"loyalty\", \"tpr_ai_rank\": null, \"tnr_ai_rank\": null, \"tpr_mean\": null, \"tnr_mean\": null, \"ba_mean\": null, \"ba_ai_rank\": null, \"tpr_ai\": 0.5307985345522562, \"tnr_ai\": 0.9137007594108582, \"ba_ai\": null, \"pi_1\": null, \"model_name\": \"Human\", \"data\": \"MFTC\", \"fpr\": 0.08629924058914185, \"fnr\": 0.46920146544774377, \"annotator\": \"Human Baseline\"}, {\"foundation\": \"sanctity\", \"tpr_ai_rank\": null, \"tnr_ai_rank\": null, \"tpr_mean\": null, \"tnr_mean\": null, \"ba_mean\": null, \"ba_ai_rank\": null, \"tpr_ai\": 0.409349004427592, \"tnr_ai\": 0.926741381486257, \"ba_ai\": null, \"pi_1\": null, \"model_name\": \"Human\", \"data\": \"MFTC\", \"fpr\": 0.07325861851374305, \"fnr\": 0.590650995572408, \"annotator\": \"Human Baseline\"}, {\"foundation\": \"authority\", \"tpr_ai_rank\": null, \"tnr_ai_rank\": null, \"tpr_mean\": null, \"tnr_mean\": null, \"ba_mean\": null, \"ba_ai_rank\": null, \"tpr_ai\": 0.4349704285462697, \"tnr_ai\": 0.8402383128801981, \"ba_ai\": null, \"pi_1\": null, \"model_name\": \"Human\", \"data\": \"eMFD\", \"fpr\": 0.15976168711980188, \"fnr\": 0.5650295714537303, \"annotator\": \"Human Baseline\"}, {\"foundation\": \"care\", \"tpr_ai_rank\": null, \"tnr_ai_rank\": null, \"tpr_mean\": null, \"tnr_mean\": null, \"ba_mean\": null, \"ba_ai_rank\": null, \"tpr_ai\": 0.4018450975418091, \"tnr_ai\": 0.8475655714670817, \"ba_ai\": null, \"pi_1\": null, \"model_name\": \"Human\", \"data\": \"eMFD\", \"fpr\": 0.15243442853291833, \"fnr\": 0.5981549024581909, \"annotator\": \"Human Baseline\"}, {\"foundation\": \"fairness\", \"tpr_ai_rank\": null, \"tnr_ai_rank\": null, \"tpr_mean\": null, \"tnr_mean\": null, \"ba_mean\": null, \"ba_ai_rank\": null, \"tpr_ai\": 0.4511113266150157, \"tnr_ai\": 0.8377394278844198, \"ba_ai\": null, \"pi_1\": null, \"model_name\": \"Human\", \"data\": \"eMFD\", \"fpr\": 0.1622605721155802, \"fnr\": 0.5488886733849843, \"annotator\": \"Human Baseline\"}, {\"foundation\": \"loyalty\", \"tpr_ai_rank\": null, \"tnr_ai_rank\": null, \"tpr_mean\": null, \"tnr_mean\": null, \"ba_mean\": null, \"ba_ai_rank\": null, \"tpr_ai\": 0.40588252743085224, \"tnr_ai\": 0.8360686103502909, \"ba_ai\": null, \"pi_1\": null, \"model_name\": \"Human\", \"data\": \"eMFD\", \"fpr\": 0.1639313896497091, \"fnr\": 0.5941174725691478, \"annotator\": \"Human Baseline\"}, {\"foundation\": \"sanctity\", \"tpr_ai_rank\": null, \"tnr_ai_rank\": null, \"tpr_mean\": null, \"tnr_mean\": null, \"ba_mean\": null, \"ba_ai_rank\": null, \"tpr_ai\": 0.388110746939977, \"tnr_ai\": 0.8646615147590637, \"ba_ai\": null, \"pi_1\": null, \"model_name\": \"Human\", \"data\": \"eMFD\", \"fpr\": 0.13533848524093628, \"fnr\": 0.6118892530600231, \"annotator\": \"Human Baseline\"}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.FacetChart(...)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate FPR and FNR\n",
    "df['fpr'] = 1 - df['tnr_ai']\n",
    "df['fnr'] = 1 - df['tpr_ai']\n",
    "df['annotator'] = df['model_name'].map({\n",
    "    'Human': 'Human Baseline',\n",
    "    'claude-4-sonnet': 'Claude-4',\n",
    "    'deepseek-v3': 'DeepSeek-V3', \n",
    "    'llama4_maverick': 'Llama4-Maverick',\n",
    "})\n",
    "df['data'] = df['data'].str.split(\"-\").str[-1]\n",
    "df_mask = df[mask]\n",
    "\n",
    "chart = alt.Chart(df_mask).mark_point(size=200, stroke='white', strokeWidth=2, filled=True).encode(\n",
    "   x=alt.X('jitter_x:Q', title='False Positive Rate', axis=alt.Axis(format='.0%'), scale=alt.Scale(domain=[0.7, 0.7])),\n",
    "   y=alt.Y('jitter_y:Q', title='False Negative Rate', axis=alt.Axis(format='.0%'), scale=alt.Scale(domain=[0.7, 0.7])),\n",
    "   color=alt.Color('foundation:N', scale=color_scale, legend=alt.Legend(title=\"Moral Dimension\")),\n",
    "   shape=alt.Shape('annotator:N',\n",
    "               scale=alt.Scale(\n",
    "                   domain=['Human Baseline', 'Claude-4', 'DeepSeek-V3', 'Llama4-Maverick'],\n",
    "                   range=['circle', 'square', 'triangle-up', 'diamond']\n",
    "               ),\n",
    "               legend=alt.Legend(title=\"Model\")),\n",
    "   tooltip=['foundation', 'annotator', 'fpr:Q', 'fnr:Q']\n",
    ").transform_calculate(\n",
    "   jitter_x='datum.fpr + (random() - 0.5) * 0.01',\n",
    "   jitter_y='datum.fnr + (random() - 0.5) * 0.01'\n",
    ").properties(\n",
    "   width=300, \n",
    "   height=200,\n",
    ")\n",
    "\n",
    "line = alt.Chart(df_mask).mark_line(color='gray', strokeDash=[2, 2]).encode(\n",
    "    x=alt.X('value:Q', scale=alt.Scale(domain=[0, 0.7])),\n",
    "    y=alt.Y('value:Q', scale=alt.Scale(domain=[0, 0.7]))\n",
    ").transform_calculate(\n",
    "    value='sequence(0, 0.8, 0.1)'\n",
    ").transform_flatten(['value'])\n",
    "\n",
    "chart = alt.layer(line, chart).facet(\n",
    "   column=alt.Column('data:N', title=\"Dataset\")\n",
    ").resolve_scale(\n",
    "   color='shared',\n",
    "   shape='shared'\n",
    ").configure_facet(\n",
    "   columns=3\n",
    ").configure_legend(\n",
    "   orient='top',\n",
    "   columns=2,  # or however many columns you want\n",
    "   symbolLimit=0  # removes symbol limit if you have many legend items\n",
    ")\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swap column levels to put fpr/fnr at second level\n",
    "pivot_df = df.pivot_table(\n",
    "  index=['data', 'annotator'], \n",
    "  columns='foundation', \n",
    "  values=['fpr', 'fnr']\n",
    ") * 100\n",
    "\n",
    "# Swap the column levels and rename\n",
    "pivot_df = pivot_df.swaplevel(0, 1, axis=1).sort_index(axis=1)\n",
    "pivot_df.columns = pivot_df.columns.set_names(['Moral Dimension', 'Metric'])\n",
    "\n",
    "# Rename FPR/FNR to uppercase\n",
    "pivot_df = pivot_df.rename(columns={'fpr': 'FPR', 'fnr': 'FNR'}, level=1)\n",
    "\n",
    "pivot_df = pivot_df.round(1)\n",
    "\n",
    "latex_table = pivot_df.to_latex(\n",
    "  multirow=True,\n",
    "  multicolumn=True,\n",
    "  escape=False,\n",
    "  column_format='ll' + 'rr' * len(pivot_df.columns.get_level_values(0).unique()),\n",
    "  float_format='%.1f'\n",
    ")\n",
    "\n",
    "pivot_df\n",
    "# print(latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrepancies\n",
    "\n",
    "Case studies of AI-human disagreements, false negatives by humans that were flagged by AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      text   subreddit  \\\n",
      "text_id                                                                  \n",
      "2002     Anyone think Macron should dispose of the Alge...  neoliberal   \n",
      "2002     Anyone think Macron should dispose of the Alge...  neoliberal   \n",
      "2002     Anyone think Macron should dispose of the Alge...  neoliberal   \n",
      "\n",
      "                  bucket    annotator     annotation confidence label  \n",
      "text_id                                                                \n",
      "2002     French politics  annotator03      Non-Moral  Confident  none  \n",
      "2002     French politics  annotator04  Thin Morality  Confident  none  \n",
      "2002     French politics  annotator02      Non-Moral  Confident  none  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "text_id  text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
       "12950    The RNC fully endorsed the Roy Moore campaign after they said they found the child molestation allegations credible                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        5\n",
       "11805    She is against the German vision of the EU where the benefits flow back to the Reich, while the rest of Europa whither under austerity Fillon, Renzi, Tsipras, etc advocate with Merkel pulling the strings. Le Pen, I would speculate, would be for a EU that exists in reality but is dysfunctional. She's advocating border controls that already exist from Austria to Denmark. The reality is Merkel unilaterally promoted a policy that resulted in millions of muslims and the majority, including Le Pen wants nothing to do with them.                                                                                                                                                                                                                            5\n",
       "3845     Fillon - criminal investigation into the use of government funds to allegedly pay his family members for doing fake jobs. The raid is part of this investigation.\\n\\nhttps://www.google.com/amp/www.bbc.co.uk/news/amp/39146848\\n\\nLe pen - Posted pictures of ISIS brutality as part of her political campaign. This goes against french speech laws, so the french government was trying to take action against her. Le pen claimed diplomatic immunity as a member of the eu parliament, which has revoked her immunity in response. She also faces similar charges to Fillon over the alleged misuse of eu funds to pay her aides. \\n\\nhttp://www.npr.org/sections/thetwo-way/2017/03/02/518127204/marine-le-pen-faces-possible-prosecution-over-graphic-tweets\\n\\n    5\n",
       "14712    Victim blaming are we? Even if the therapist is her friend, she should've known not to tell her anything. It's immoral and extremely disrespectful, and being friends with his wife is no excuse.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          5\n",
       "5112     Hybristophilia is a paraphilia in which sexual arousal, facilitation, and attainment of orgasm are responsive to and contingent upon being with a partner known to have committed an outrage, cheating, lying, known infidelities, or crime—such as rape or murder. \\n\\nSo sadly, it exists.\\n\\nI'll never understand it, hopefully, I never will.                                                                                                                                                                                                                                                                                                                                                                                                                         5\n",
       "12959    The Swedish media is suppressing the nationality of their rapists and people like you are acting as apologists for it, you need to be stopped. Trump, Brexit, Le Pen...is a movement against this.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         5\n",
       "10269    No no, mean and angry is something only Republicans are allowed to be. If you're mean and angry, then you're on the wrong side of Shroedinger's liberal, a heathen that has disavowed civility. You must sit and accept injustice lest you cause someone to clutch their pearls too hard.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  5\n",
       "13833    They'll be planning Michigan-like, Sovereign citizen batshit insurrection plots for a generation.\\n\\nFortunately, they they're largely incompetent and can't keep their fucking mouths shut just like their GodEmperorDaddyFuhrer.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         4\n",
       "13386    The usual shitting on about Macron snuffing out democracy, claims there will be no democratic mandate to \"destroy out labour unions, our environment, and our regulations\". \\n\\nHe may as well campaign for FN, the cunt.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  4\n",
       "16629    You know what? I don't mind the hard drugs, the Islamification and obvious support for ISIS and what not, but colliding with ANTIFA? How are they colliding, is it bumper cars or are we talking about a Large Macron Collider here? That shit's dangerous.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                4\n",
       "12948    The President of the United States is in debt to someone.  400 million to 1.1 Billion.  A massive security clearance violation as it could put the US Tax Payer on the hook for the presidents debts.\\n\\nPresidential bankruptcy.  If he wins it comes due in two years                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    4\n",
       "5283     I believe what /u/Rightquercusalba mean is that the tides are shifting in favor of *actual conservatives* instead of fake-conservative GOPe trash that are perfectly happy to sell us out so long as they keep getting their invites to the liberal elites' parties.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       4\n",
       "12296    Step right up folks and treat yourself to Trump's magical Coranvirus elixir.  It consists of blend of heat, disinfectant and UV rays.  You're wiping down your groceries with disinfectant aren't you? So what harm could an IV filled with sodium hypochlorite do to scrub down your insides. Then kick back under a blast of  UV rays in a room at 190 degrees Fahrenheit and say goodbye to Coronavirus.  You heard it here  first, folks.  It's what people are saying.                                                                                                                                                                                                                                                                                                4\n",
       "4087     From what I hear from a *lot* of people here, Macron represents finance, banks, soul-crushing capitalism, etc. Left-leaning and \"anti-system\" people viscerally hate him.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  4\n",
       "7464     If you care about this cat so much and it has been a WEEK since you have seen her, WHY HAVE YOU NOT CALLED THE POLICE AND REPORTED THE THEFT??? Once you do all that, you need to cut contact with your “boyfriend”. I would’ve had my cat back day of or would’ve filed the theft immediately. This is pushing ESH because of how long you’ve waited for YOUR CAT.                                                                                                                                                                                                                                                                                                                                                                                                        4\n",
       "4013     France is on a roll these days. First, they elect Macron, who actually stood up to Trump and offered sanctuary for our scientists, then they actually charge a political figure for corruption. Wow, this must be the upside down because the US is literally doing the exact opposite. I guess this makes up for those French surrender jokes.                                                                                                                                                                                                                                                                                                                                                                                                                            4\n",
       "11401    Published in 1997, laying the blueprints for Russia's return over the next several decades:\\n\\nhttps://en.wikipedia.org/wiki/Foundations_of_Geopolitics\\n\\nHere's a good read that details the motivation and psyops tactics that the Russians have been employing. There is enough circumstantial evidence connecting the campaigns of trump and Le Pen to Russia that you really have to do some mental gymnastics to explain it away. \\n\\nWe will probably never have a smoking gun, that's just not how espionage works. We have a country openly destabilizing western elections and alliances. Ignoring it because it doesn't support your narrative is a foolish choice at this point.                                                                              4\n",
       "17228    gather evidence, sue him for all he's worth :D\\n\\nno seriously though, tell him you know about it, make sure he understands what a pathetic piece of shit move this is, then just walk out.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                4\n",
       "15860    YTA, and honestly, this is pretty much standard FOG (fear obligation guilt) in terms of how you deal with your mother.\\n\\nIt's probably come up in other comments, but you should check out the \"justnomil\" subreddit.  I think you're going to find a lot of unexpected information there regarding why your ex is now your ex.  If you want to be able to find a partner in the future, or hope to get your ex back, you'll want to educate yourself.                                                                                                                                                                                                                                                                                                                    4\n",
       "12973    The accumulated filth of all their climate denial and corporate lobbyism will foam up about their waists, and all the whores and politicians will look up to Merkel and Macron and shout, \"Save us!\"... and they'll whisper \"*no.*\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        4\n",
       "dtype: object"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = pd.Series(ds_test['text']).str.contains(\"Anyone think Macron\").argmax().astype(int)\n",
    "idx = ds_test.select([idx])['text_id'][0]\n",
    "print(ds_test.to_pandas().set_index('text_id').loc[idx])\n",
    "\n",
    "pattern = '|'.join(moral_targets)\n",
    "y_true = annots.stack().str.lower().str.contains(pattern).unstack()\n",
    "(y_pred.sum(1) - y_true.sum(1)).sort_values(ascending=False).head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
