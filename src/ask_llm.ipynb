{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "The notebook evaluates different LLMs on moral foundation classification using Haidt's Moral Foundations Theory. \n",
    "\n",
    "It tests models' ability to identify five moral foundations (care/harm, fairness/cheating, loyalty/betrayal, authority/subversion, sanctity/degradation) in text samples and compares their performance using accuracy metrics logged to Weights & Biases.\n",
    "\n",
    "Run through `Papermill` with custom parameters:\n",
    "```bash\n",
    "papermill ask_llm.ipynb output.ipynb -p model_name \"llama3_8b\" -p test_data 'morality-MFRC' -p sample 10 -p temperature 0.3 --log-output\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "\n",
    "import replicate\n",
    "from datasets import Dataset, load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config\n",
    "\n",
    "Model selection, dataset parameters, temperature settings, and classification prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "model_name = 'qwen3-235b' # llama3_8b, deepseek-v3, claude-3-5-sonnet, claude-4-sonnet, gpt5-mini, qwen3-235b\n",
    "demo = 'false'\n",
    "sample = 100\n",
    "test_data = 'morality-eMFD' # morality-MFRC, morality-MFTC, morality-eMFD\n",
    "temperature = 0.3\n",
    "random_state = 42\n",
    "use_async = True\n",
    "prompt = \"\"\"You are an expert in moral psychology, classifying text according to Haidt's theory.\n",
    "            For each moral foundation, mark true if moral values from that foundation are expressed in the text, false if not expressed.\n",
    "                                                   \n",
    "            Answer only with a valid JSON in this format:\n",
    "            {\n",
    "                \"care/harm\": [true / false],\n",
    "                \"fairness/cheating\": [true / false],\n",
    "                \"loyalty/betrayal\": [true / false],\n",
    "                \"authority/subversion\": [true / false],\n",
    "                \"sanctity/degradation\": [true / false],\n",
    "            }\n",
    "            \"\"\"\n",
    "            #             Provide an explanation.\n",
    "            \n",
    "            # Haidt's foundations are:\n",
    "            #     \"care/harm\": This foundation is related to our long evolution as mammals with attachment systems and an ability to feel (and dislike) the pain of others. It underlies virtues of kindness, gentleness, and nurturance.\n",
    "            #     \"fairness/cheating\": This foundation is related to the evolutionary process of reciprocal altruism. It generates ideas of justice, rights, and autonomy.\n",
    "            #     \"loyalty/betrayal\":  This foundation is related to our long history as tribal creatures able to form shifting coalitions. It underlies virtues of patriotism and self-sacrifice for the group. It is active anytime people feel that it’s “one for all, and all for one.”\n",
    "            #     \"authority/subversion\": This foundation was shaped by our long primate history of hierarchical social interactions. It underlies virtues of leadership and followership including deference to legitimate authority and respect for traditions.\n",
    "            #     \"sanctity/degradation\": This foundation was shaped by the psychology of disgust and contamination. It underlies religious notions of striving to live in an elevated, less carnal, more noble way. It underlies the widespread idea that the body is a temple that can be desecrated by immoral activities and contaminants (an idea not unique to religious traditions).\n",
    "\n",
    "\n",
    "#                 \"reasoning\": [justification]\n",
    "# papermill ask_llm.ipynb output.ipynb -p model_name \"llama3_8b\" -p test_data 'morality-MFRC' -p sample 10 -p temperature 0.7 --log-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.init(project='morality-llm',)\n",
    "wandb.config.update({\n",
    "   'model_name': model_name,\n",
    "   'demo': demo,\n",
    "   'sample': sample,\n",
    "   'test_data': test_data,\n",
    "   'temperature': temperature,\n",
    "   'random_state': random_state,\n",
    "   'prompt': prompt\n",
    "})\n",
    "\n",
    "# RESUME\n",
    "# import wandb\n",
    "# wandb.init(project='morality-llm', id=\"1f1usujv\", resume=\"must\")\n",
    "# run = wandb.Api().run(\"morality-llm/1f1usujv\")\n",
    "# for key, value in run.config.items():\n",
    "#    globals()[key] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMs\n",
    "\n",
    "API integrations for various language models (Llama, DeepSeek, Claude, OpenAI, Qwen) with async processing capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from pprint import pprint\n",
    "\n",
    "def analyze_with_llama3_8b(text: str, temperature=temperature):\n",
    "   system_prompt = prompt\n",
    "\n",
    "   output = replicate.run(\n",
    "      \"meta/meta-llama-3-8b-instruct\",\n",
    "      input={\n",
    "         \"system_prompt\": system_prompt,\n",
    "         \"prompt\": f'Text: \"{text}\"',\n",
    "         \"max_new_tokens\": 200,\n",
    "         # \"temperature\": temperature,\n",
    "         \"return_full_text\":False\n",
    "      }\n",
    "   )\n",
    "\n",
    "   \n",
    "   response_text = \"\".join(output).strip()\n",
    "   # json_start = response_text.find('[')\n",
    "   # json_end = response_text.rfind(']') + 1\n",
    "   # json_str = response_text[json_start:json_end]\n",
    "   # #clean = MoralAnalysis(**json.loads(json_str))\n",
    "   # clean = json.loads(json_str)\n",
    "#    print(output)\n",
    "\n",
    "   return response_text\n",
    "\n",
    "\n",
    "def analyze_with_llama4_maverick(text: str, temperature=temperature):\n",
    "   system_prompt = prompt\n",
    "   output = replicate.run(\n",
    "      \"meta/llama-4-maverick-instruct\",\n",
    "      input={\n",
    "         # \"system_prompt\": system_prompt,\n",
    "         \"prompt\": f'{system_prompt}\\n Summarize reasoning after briefly. \\n Text: \"{text}\"',\n",
    "         \"max_tokens\": 100,\n",
    "         \"temperature\": temperature,\n",
    "         \"return_full_text\":False\n",
    "      }\n",
    "   )\n",
    "   \n",
    "   response_text = \"\".join(output).strip()\n",
    "   return response_text\n",
    "\n",
    "\n",
    "async def analyze_with_llama4_maverick_async(text: str, temperature=0.3):\n",
    "   output = await asyncio.to_thread(\n",
    "      analyze_with_llama4_maverick,\n",
    "      text=text,\n",
    "      temperature=temperature,\n",
    "      )\n",
    "   return output\n",
    "\n",
    "\n",
    "# Usage\n",
    "texts = [\n",
    "   # \"We must protect our children from harm\",\n",
    "   # \"Everyone deserves equal pay for equal work\",\n",
    "   # \"Stand with our troops who sacrifice for freedom\",\n",
    "   # \"Respect your elders and traditional values\",\n",
    "   # \"Marriage is sacred and must be protected\",\n",
    "   # \"The police officer arrested a peacefully protesting young woman\",\n",
    "   # \"My cat likes eating raw beef meet.\",\n",
    "   # \"Anyone think Macron should dispose of the Algerian dictator? France did little to support the uprsings of 2010-2012.\"\n",
    "]\n",
    "\n",
    "# DEMO\n",
    "for text in texts:\n",
    "   try:\n",
    "       result = await analyze_with_llama4_maverick_async(text)\n",
    "       pprint(f\"\\nText: {text}\")\n",
    "       pprint(f\"Moral content: {result}\")\n",
    "      #  print(f\"Moral content: {result.overall_moral_content}\")\n",
    "      #  for foundation in result.foundations:\n",
    "      #      print(f\"  {foundation.foundation}\")\n",
    "      #    #   print(f\"  {foundation.foundation}: - {foundation.explanation}\")\n",
    "   except Exception as e:\n",
    "       pprint(f\"Error: '{text}' → {e}\")\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "def analyze_with_deepseek(text: str, temperature=temperature):\n",
    "    output = replicate.run(\n",
    "        \"deepseek-ai/deepseek-v3\",\n",
    "        input={\n",
    "            \"prompt\": prompt+f\"\"\"\\nText: {text}.\"\"\",\n",
    "            \"max_new_tokens\": 200,\n",
    "            \"temperature\": temperature\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    response_text = \"\".join(output).strip()\n",
    "\n",
    "    return response_text\n",
    "\n",
    "async def analyze_with_deepseek_async(text: str, temperature=0.3):\n",
    "   output = await asyncio.to_thread(\n",
    "       replicate.run,\n",
    "       \"deepseek-ai/deepseek-v3\",\n",
    "        input={\n",
    "            \"prompt\": prompt+f\"\"\"\\nText: {text}.\"\"\",\n",
    "            \"max_new_tokens\": 200,\n",
    "            \"temperature\": temperature\n",
    "        }\n",
    "        )\n",
    "\n",
    "   response = ''.join(output)\n",
    "   return response\n",
    "\n",
    "text = \"Anyone think Macron should dispose of the Algerian dictator? France did little to support the uprsings of 2010-2012.\"\n",
    "# text = \"The CEO received a $10 million bonus while laying off workers.\"\n",
    "# text = \"My cat eats raw meet.\"\n",
    "# text = \"The marrige should be protected\"\n",
    "# analyze_with_deepseek(text)\n",
    "# await analyze_with_deepseek_async(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "\n",
    "def analyze_with_claude(text: str, temperature=0.7):\n",
    "    client = anthropic.Anthropic()\n",
    "    output = client.messages.create(\n",
    "        # model=\"claude-3-5-sonnet-20241022\",\n",
    "        model=\"claude-4-sonnet-20250514\",\n",
    "        max_tokens=250,\n",
    "        temperature=temperature,\n",
    "        system=prompt+\"+ brief reasoning\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": f\"Text: {text}\"}\n",
    "        ]\n",
    "    )\n",
    "    response_text = output.content[0].text.strip()\n",
    "    \n",
    "    return response_text\n",
    "\n",
    "import anthropic\n",
    "from tqdm.asyncio import tqdm as tqdm_async\n",
    "\n",
    "async def analyze_with_claude_async(text: str,  temperature=0.7):\n",
    "    client=anthropic.AsyncAnthropic()\n",
    "    output = await client.messages.create(\n",
    "        model=\"claude-4-sonnet-20250514\",\n",
    "        max_tokens=250,\n",
    "        temperature=temperature,\n",
    "        # system=prompt,\n",
    "        system=[\n",
    "            {\"type\": \"text\", \"text\": prompt, \"cache_control\": {\"type\": \"ephemeral\"}}\n",
    "        ],\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": f\"Text: {text}\"}\n",
    "        ]\n",
    "    )\n",
    "    response_text = output.content[0].text.strip()\n",
    "    \n",
    "    return response_text\n",
    "\n",
    "# Usage\n",
    "text = \"Anyone think Macron should dispose of the Algerian dictator? France did little to support the uprsings of 2010-2012.\"\n",
    "# text = \"Immigrants make up 15% of the workforce in this sector.\"\n",
    "# text = \"The CEO received a $10 million bonus while laying off workers.\"\n",
    "# text = \"The church collects $50,000 weekly in donations.\"\n",
    "response = analyze_with_claude(text)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "import asyncio\n",
    "\n",
    "def analyze_with_openai(text: str, temperature=0.3, model_name=\"gpt-5-nano\"):\n",
    "   client = AzureOpenAI(api_version = \"2024-02-15-preview\", azure_endpoint =\"https://murmur.openai.azure.com/\")\n",
    "   response = client.chat.completions.create(\n",
    "      model=model_name,  \n",
    "      messages=[\n",
    "         {\"role\": \"system\", \"content\": prompt},\n",
    "         {\"role\": \"user\", \"content\": f'Text: {text}'}\n",
    "      ],\n",
    "      # temperature=0.3,\n",
    "      # max_completion_tokens=450,\n",
    "   )\n",
    "   response = response.choices[0].message.content\n",
    "   return response\n",
    "\n",
    "\n",
    "async def analyze_with_openai_async(text: str, temperature=0.3, model_name=\"gpt-5-nano\"):\n",
    "   response = await asyncio.to_thread(\n",
    "        analyze_with_openai,\n",
    "         {\n",
    "            'text':text,\n",
    "            'model_name':model_name,\n",
    "            'temperature':temperature,\n",
    "         }\n",
    "   )\n",
    "   return response\n",
    "\n",
    "\n",
    "text = \"Anyone think Macron should dispose of the Algerian dictator? France did little to support the uprsings of 2010-2012.\"\n",
    "# text = \"The CEO received a $10 million bonus while laying off workers.\"\n",
    "response = await analyze_with_openai_async(text)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_with_qwen3235b(text: str, temperature=0.3):\n",
    "   output = replicate.run(\n",
    "      \"qwen/qwen3-235b-a22b-instruct-2507\",\n",
    "      input={\n",
    "         # \"system_prompt\": system_prompt,\n",
    "         \"prompt\": prompt+ \"\\n Summarize reasoning after briefly. \\n\" + f'text: {text}',\n",
    "         \"max_tokens\": 150,\n",
    "         \"temperature\": temperature,\n",
    "         \"return_full_text\": False\n",
    "      }\n",
    "   )\n",
    "    \n",
    "   response = ''.join(output)\n",
    "   return response\n",
    "\n",
    "async def analyze_with_qwen3235b_async(text: str, temperature=0.3):\n",
    "   response = await asyncio.to_thread(\n",
    "        analyze_with_qwen3235b,\n",
    "       {\"text\": text,\n",
    "        \"temperature\":temperature,\n",
    "        }\n",
    "   )\n",
    "   return response\n",
    "\n",
    "\n",
    "text = \"Anyone think Macron should dispose of the Algerian dictator? France did little to support the uprsings of 2010-2012.\"\n",
    "# text = \"The CEO received a $10 million bonus while laying off workers.\"\n",
    "# response = await analyze_with_qwen3235b_async(text)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Dataset loading, preprocessing, and sample selection for moral foundation testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "ds_test = load_dataset(f\"maciejskorski/{test_data}\")['train']\n",
    "\n",
    "labels = ds_test.to_pandas().groupby('text_id')['label'].apply(';'.join)\n",
    "texts = ds_test.to_pandas().groupby('text_id')['text'].first()\n",
    "\n",
    "ds_test = Dataset.from_dict({'id': labels.index, 'text':texts, 'label': labels})\n",
    "\n",
    "if sample > 0:\n",
    "    _,test_idxs = train_test_split(range(ds_test.num_rows), test_size=sample, shuffle=True, random_state=random_state)\n",
    "else:\n",
    "    # sample = len(ds_test)\n",
    "    test_idxs = np.arange(ds_test.num_rows)\n",
    "\n",
    "ds_test_sample = ds_test.select(test_idxs)\n",
    "display(ds_test_sample.select(range(5)).to_pandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing\n",
    "\n",
    "Batch processing pipeline with error handling and concurrent request management for LLM inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CALL_FUNCTIONS = {\n",
    "    'llama3_8b': analyze_with_llama3_8b, \n",
    "    'llama4_maverick':  analyze_with_llama4_maverick_async if use_async else analyze_with_llama4,\n",
    "    'deepseek-v3': analyze_with_deepseek_async if use_async else analyze_with_deepseek,\n",
    "    'claude-4-sonnet':analyze_with_claude_async if use_async else analyze_with_claude,\n",
    "    'gpt-5-nano': analyze_with_openai_async if use_async else analyze_with_openai,\n",
    "    'gpt-5-mini': analyze_with_openai_async if use_async else analyze_with_openai,\n",
    "    'qwen3-235b': analyze_with_qwen3235b_async if use_async else analyze_with_openai_async\n",
    "}\n",
    "\n",
    "analyze_with_llm = CALL_FUNCTIONS.get(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import asyncio\n",
    "\n",
    "semaphore = asyncio.Semaphore(10)  # Max concurrent requests\n",
    "\n",
    "async def process_async(example):\n",
    "    async with semaphore:\n",
    "        try:\n",
    "            idx = example['id']\n",
    "            response = await analyze_with_llm(example['text'])\n",
    "            out = {\"index\": idx, \"predictions\": response}\n",
    "        except Exception as e:\n",
    "            out = {\"index\": idx, \"predictions\": \"\", \"error\": str(e)}\n",
    "        return out\n",
    "\n",
    "async def process_all_async():\n",
    "    tasks = list(map(process_async, ds_test_sample))\n",
    "    results = []\n",
    "    with open(f'results_{wandb.run.id}.jsonl', 'w') as f:\n",
    "        for task in tqdm_async.as_completed(tasks, file=sys.stdout):\n",
    "            result = await task\n",
    "            results.append(result)\n",
    "            f.write(json.dumps(result) + '\\n')\n",
    "    return results\n",
    "\n",
    "def process(example):\n",
    "    idx = example['id']\n",
    "    response = analyze_with_llm(example['text'])\n",
    "    out = {\"index\": idx, \"predictions\": response}\n",
    "    return out\n",
    "\n",
    "def process_all():\n",
    "    tasks = map(process, ds_test_sample)\n",
    "    results = []\n",
    "    for task in tqdm(tasks, file=sys.stdout, total = len(ds_test_sample)):\n",
    "        result = task\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "if use_async:\n",
    "    results = await process_all_async()\n",
    "else:\n",
    "    results = process_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.json_normalize(results) \n",
    "df_results['label'] = pd.Series(ds_test_sample['label'],index=ds_test_sample['id']).loc[df_results['index']].values\n",
    "df_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({\"predictions\": wandb.Table(dataframe=df_results)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "moral_targets = ['care', 'fairness', 'loyalty', 'authority', 'sanctity']\n",
    "pattern = re.compile(r':\\s*\\[?(true|false)\\]?', re.IGNORECASE)\n",
    "\n",
    "def extract_booleans(text):\n",
    "   # Find true/false values in order\n",
    "   matches = re.findall(r':\\s*(true|false)', text.lower())[:5]\n",
    "   return [m == 'true' for m in matches]\n",
    "\n",
    "# bool_lists = df_results['predictions'].apply(extract_booleans)\n",
    "bool_lists = df_results['predictions'].str.findall(pattern).str[:5].apply(lambda x: [s == 'true' for s in x])\n",
    "\n",
    "y_pred = pd.DataFrame(bool_lists.tolist(), columns=moral_targets).astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = df_results\n",
    "y_true = pd.DataFrame( zip(*[out['label'].str.contains(target) for target in moral_targets]), columns=moral_targets )\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "report = classification_report(y_true, y_pred, \n",
    "                            target_names=moral_targets, \n",
    "                            output_dict=True, \n",
    "                            zero_division=0)\n",
    "report = pd.DataFrame(report).T\n",
    "report.index.name = \"target\"\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.log({\"accuracy\": wandb.Table(dataframe=report.reset_index())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "# import pandas as pd\n",
    "# api = wandb.Api()\n",
    "# run_id = \"ftz7ti3p\"\n",
    "# project_name = \"morality-llm\"\n",
    "# # run = api.run(f\"{project_name}/{run_id}\")\n",
    "# artifact = api.artifact(f\"{project_name}/run-{run_id}-accuracy:latest\")\n",
    "\n",
    "# # modify the artifact, here add an index\n",
    "# table = artifact.get(\"accuracy\")\n",
    "# table = pd.DataFrame(table.data,columns=table.columns)\n",
    "# table.index = ['care', 'fairness', 'loyalty', 'authority', 'sanctity', 'micro avg', 'macro avg', 'weighted avg', 'samples avg']\n",
    "# table.index.name = \"target\"\n",
    "\n",
    "# # upload it\n",
    "# with wandb.init(project=project_name, id=run_id, resume=\"allow\"):\n",
    "#     wandb.log({\"accuracy\": wandb.Table(dataframe=table.reset_index())})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
